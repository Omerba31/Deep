{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Omerba31/Deep/blob/main/final_project_omerVer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
        "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.cluster import DBSCAN\n"
      ],
      "metadata": {
        "id": "FHaWSKkKY9kJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#     Basic Definitions\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "eval_mode = False\n",
        "epochs = 5\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "embedding_dim = 64\n",
        "triplet_alpha = 1.0  # Weight for Triplet Loss\n",
        "\n",
        "num_ood_val = 300\n",
        "num_ood_test = 800"
      ],
      "metadata": {
        "id": "QVqL7KPMZIlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4265756f-289a-4b24-ce50-9b93a478fd5a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Merges two datasets (MNIST for in-dist, and OOD) => label=10 for OOD\n",
        "    \"\"\"\n",
        "    def __init__(self, ds_in, ds_ood):\n",
        "        self.ds_in = ds_in\n",
        "        self.ds_ood = ds_ood\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds_in) + len(self.ds_ood)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < len(self.ds_in):\n",
        "            data, label = self.ds_in[idx]\n",
        "            return data, label\n",
        "        else:\n",
        "            data, _ = self.ds_ood[idx - len(self.ds_in)]\n",
        "            return data, 10"
      ],
      "metadata": {
        "id": "Ab9Y6HYVZVZP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluates an OSR model that outputs (N, 11) logits\n",
        "    (10 known classes + 1 'unknown').\n",
        "    Splits predictions for MNIST (label<10) vs OOD (label=10)\n",
        "    and returns separate accuracies plus overall accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct_mnist = 0\n",
        "    total_mnist = 0\n",
        "    correct_ood = 0\n",
        "    total_ood = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in data_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)  # shape (N,11)\n",
        "            _, y_pred = torch.max(outputs, dim=1)\n",
        "\n",
        "            mask_in = (labels < 10)\n",
        "            mask_ood = (labels == 10)\n",
        "\n",
        "            labels_in = labels[mask_in]\n",
        "            labels_ood = labels[mask_ood]\n",
        "            pred_in = y_pred[mask_in]\n",
        "            pred_ood = y_pred[mask_ood]\n",
        "\n",
        "            total_mnist += labels_in.size(0)\n",
        "            correct_mnist += (pred_in == labels_in).sum().item()\n",
        "\n",
        "            total_ood += labels_ood.size(0)\n",
        "            correct_ood += (pred_ood == labels_ood).sum().item()\n",
        "\n",
        "    acc_mnist = correct_mnist / total_mnist if total_mnist>0 else 0\n",
        "    acc_ood   = correct_ood / total_ood     if total_ood>0   else 0\n",
        "    acc_total = (correct_mnist + correct_ood) / (total_mnist + total_ood)\n",
        "    return acc_mnist, acc_ood, acc_total\n"
      ],
      "metadata": {
        "id": "-g4x7uq6ZXtw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_EmbeddingNet_Triplet(nn.Module):\n",
        "    def __init__(self, embedding_dim=64, dropout_p=0.3):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        # Convs\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "        # FC\n",
        "        self.fc1 = nn.Linear(64*7*7, 128)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc2 = nn.Linear(128, embedding_dim)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(embedding_dim, 10)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        emb = self.fc2(x)\n",
        "        return emb\n",
        "\n",
        "    def forward_classifier(self, emb):\n",
        "        return self.classifier(emb)\n",
        "\n",
        "    def forward_inference_10(self, x):\n",
        "        emb = self.forward_features(x)\n",
        "        logits = self.forward_classifier(emb)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.forward_inference_10(x)\n"
      ],
      "metadata": {
        "id": "7WtnpjA-dVvC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch_triplet(model, loader, optimizer, ce_criterion, dbscan_clusters, triplet_margin=1.0, alpha=1.0,\n",
        "                            negative_sampling_strategy=\"different_class_same_cluster\"):\n",
        "    \"\"\"\n",
        "    Trains the model using a combined Cross-Entropy + Triplet loss.\n",
        "    Uses `compute_dbscan_clusters` output for negative selection and dynamic regularization.\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    triplet_criterion = nn.TripletMarginLoss(margin=triplet_margin, reduction='mean')\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_ce = 0.0\n",
        "    running_trip = 0.0\n",
        "    total_samples = 0\n",
        "    correct = 0\n",
        "\n",
        "    for data, labels in loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model.forward_inference_10(data)\n",
        "        ce_loss = ce_criterion(logits, labels)\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        emb = model.forward_features(data)\n",
        "\n",
        "        anchor_list = []\n",
        "        pos_list = []\n",
        "        neg_list = []\n",
        "        triplet_weights = []\n",
        "\n",
        "        label2indices = {}\n",
        "        for i, lab in enumerate(labels):\n",
        "            lab_i = lab.item()\n",
        "            label2indices.setdefault(lab_i, []).append(i)\n",
        "\n",
        "        for i, lab in enumerate(labels):\n",
        "            anchor_label = lab.item()\n",
        "\n",
        "            # Get clusters for this class\n",
        "            cluster_data = dbscan_clusters.get(anchor_label, [])\n",
        "\n",
        "            # Assign anchor to nearest cluster (by centroid distance)\n",
        "            anchor_cluster = None\n",
        "            if cluster_data:\n",
        "                anchor_embedding = emb[i].detach().cpu().numpy()  # ✅ FIXED: detach() before .numpy()\n",
        "                cluster_distances = [np.linalg.norm(anchor_embedding - cluster[\"centroid\"]) for cluster in cluster_data]\n",
        "                anchor_cluster = np.argmin(cluster_distances)  # Nearest cluster index\n",
        "\n",
        "            # Select positive from the same class (nearest cluster)\n",
        "            pos_idx = i  # Default to self if no better option\n",
        "            if cluster_data:\n",
        "                pos_candidates = label2indices[anchor_label]\n",
        "                pos_idx = random.choice(pos_candidates) if len(pos_candidates) > 1 else i\n",
        "\n",
        "            # Select negative\n",
        "            neg_idx = None\n",
        "\n",
        "            if negative_sampling_strategy == \"same_class_different_cluster\":\n",
        "                # Pick negative from a different cluster in the same class\n",
        "                neg_candidates = []\n",
        "                for cluster_id, cluster_info in enumerate(cluster_data):\n",
        "                    if cluster_id != anchor_cluster:\n",
        "                        neg_candidates.extend(label2indices.get(anchor_label, []))  # Same class, different cluster\n",
        "\n",
        "                if neg_candidates:\n",
        "                    neg_idx = random.choice(neg_candidates)\n",
        "                else:\n",
        "                    # Fallback: Choose negative from a different class\n",
        "                    other_classes = [cls for cls in dbscan_clusters.keys() if cls != anchor_label]\n",
        "                    if other_classes:\n",
        "                        random_class = random.choice(other_classes)\n",
        "                        neg_idx = random.choice(label2indices[random_class])\n",
        "\n",
        "            elif negative_sampling_strategy == \"different_class_same_cluster\":\n",
        "                # Pick negative from a different class but in the same closest cluster\n",
        "                neg_candidates = []\n",
        "                for other_class, clusters in dbscan_clusters.items():\n",
        "                    if other_class != anchor_label and anchor_cluster < len(clusters):\n",
        "                        neg_candidates.extend(label2indices.get(other_class, []))  # Different class, same cluster\n",
        "\n",
        "                if neg_candidates:\n",
        "                    neg_idx = random.choice(neg_candidates)\n",
        "                else:\n",
        "                    # Fallback: Choose negative from a different class\n",
        "                    other_classes = [cls for cls in dbscan_clusters.keys() if cls != anchor_label]\n",
        "                    if other_classes:\n",
        "                        random_class = random.choice(other_classes)\n",
        "                        neg_idx = random.choice(label2indices[random_class])\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Invalid negative_sampling_strategy. Choose 'same_class_different_cluster' or 'different_class_same_cluster'.\")\n",
        "\n",
        "            # Apply dynamic loss weighting using cluster radius\n",
        "            if cluster_data:\n",
        "                cluster_radius = cluster_data[anchor_cluster][\"radius\"] if anchor_cluster is not None else 1.0\n",
        "            else:\n",
        "                cluster_radius = 1.0  # Default if no cluster data available\n",
        "            dynamic_weight = 1.0 / (cluster_radius + 1e-5)  # Normalize loss weight\n",
        "            triplet_weights.append(dynamic_weight)\n",
        "\n",
        "            if neg_idx is not None:\n",
        "                anchor_list.append(emb[i].unsqueeze(0))\n",
        "                pos_list.append(emb[pos_idx].unsqueeze(0))\n",
        "                neg_list.append(emb[neg_idx].unsqueeze(0))\n",
        "\n",
        "        # Compute triplet loss only if valid triplets exist\n",
        "        if len(anchor_list) > 0:\n",
        "            anchor_t = torch.cat(anchor_list, dim=0)\n",
        "            pos_t = torch.cat(pos_list, dim=0)\n",
        "            neg_t = torch.cat(neg_list, dim=0)\n",
        "            trip_loss = triplet_criterion(anchor_t, pos_t, neg_t)\n",
        "\n",
        "            # Apply dynamic weighting\n",
        "            weighted_trip_loss = torch.tensor(triplet_weights, device=device).mean() * trip_loss\n",
        "        else:\n",
        "            weighted_trip_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "        # Compute total loss\n",
        "        total_loss = ce_loss + alpha * weighted_trip_loss\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track losses\n",
        "        running_loss += total_loss.item() * data.size(0)\n",
        "        running_ce += ce_loss.item() * data.size(0)\n",
        "        running_trip += weighted_trip_loss.item() * data.size(0)\n",
        "\n",
        "    ep_loss = running_loss / total_samples\n",
        "    ep_ce = running_ce / total_samples\n",
        "    ep_trip = running_trip / total_samples\n",
        "    ep_acc = correct / total_samples\n",
        "    return ep_loss, ep_ce, ep_trip, ep_acc\n"
      ],
      "metadata": {
        "id": "nHkGcoaIZhAH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_classifier(model, loader, ce_criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels in loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            logits = model.forward_inference_10(data)\n",
        "            ce_loss = ce_criterion(logits, labels)\n",
        "\n",
        "            total_loss += ce_loss.item() * data.size(0)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return total_loss/total, correct/total"
      ],
      "metadata": {
        "id": "byaasxxdZipM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_dbscan_clusters(model, dataset, eps, min_samples):\n",
        "    \"\"\"\n",
        "    For each class c in 0..9, gather embeddings and run DBSCAN with (eps,min_samples).\n",
        "    Return a dict c -> list of (centroid, radius) for each cluster.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    label2vecs = {c: [] for c in range(10)}\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
        "    with torch.no_grad():\n",
        "        for data, labels in loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            emb = model.forward_features(data)\n",
        "            emb_np = emb.cpu().numpy()  # shape (B,embedding_dim)\n",
        "            for i, lab in enumerate(labels):\n",
        "                label2vecs[lab.item()].append(emb_np[i])\n",
        "\n",
        "    dbscan_dict = {}\n",
        "    for c in range(10):\n",
        "        arr = np.vstack(label2vecs[c])  # (#samples_in_class,embedding_dim)\n",
        "        if len(arr) == 0:\n",
        "            dbscan_dict[c] = []\n",
        "            continue\n",
        "\n",
        "        dbs = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        cluster_labels = dbs.fit_predict(arr)\n",
        "\n",
        "        clusters_info = []\n",
        "        unique_labels = set(cluster_labels) - {-1}\n",
        "        for clab in unique_labels:\n",
        "            points_in_cluster = arr[cluster_labels == clab]\n",
        "            centroid = points_in_cluster.mean(axis=0)\n",
        "            dists = np.sqrt(((points_in_cluster - centroid)**2).sum(axis=1))\n",
        "            radius = dists.max()\n",
        "            clusters_info.append({\n",
        "                \"centroid\": centroid,\n",
        "                \"radius\": radius\n",
        "            })\n",
        "        dbscan_dict[c] = clusters_info\n",
        "\n",
        "    return dbscan_dict\n"
      ],
      "metadata": {
        "id": "lwRqS5NzvouK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OSRWrapperDBSCAN(nn.Module):\n",
        "    def __init__(self, core_model, dbscan_dict, prob_threshold=0.7, dist_factor=1.0):\n",
        "        \"\"\"\n",
        "        1) If maxProb < prob_threshold => unknown\n",
        "        2) Else find nearest cluster (by centroid) in predicted class -> check distance < radius*dist_factor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.core_model = core_model\n",
        "        self.dbscan_dict = dbscan_dict\n",
        "        self.prob_threshold = prob_threshold\n",
        "        self.dist_factor = dist_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.core_model.forward_features(x)\n",
        "        logits_10 = self.core_model.forward_classifier(emb)\n",
        "        probs_10 = F.softmax(logits_10, dim=1)\n",
        "\n",
        "        out_11 = torch.zeros(x.size(0), 11, device=x.device)\n",
        "        out_11[:, :10] = logits_10\n",
        "\n",
        "        max_probs, pred_c = torch.max(probs_10, dim=1)\n",
        "        emb_np = emb.detach().cpu().numpy()\n",
        "\n",
        "        for i in range(x.size(0)):\n",
        "            p = max_probs[i].item()\n",
        "            c = pred_c[i].item()\n",
        "\n",
        "            if p < self.prob_threshold:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "                continue\n",
        "\n",
        "            clusters_info = self.dbscan_dict[c]\n",
        "            if len(clusters_info) == 0:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "                continue\n",
        "\n",
        "            sample_emb = emb_np[i]\n",
        "            min_dist = 1e9\n",
        "            best_rad = 0.0\n",
        "            for info in clusters_info:\n",
        "                centroid = info[\"centroid\"]\n",
        "                radius = info[\"radius\"]\n",
        "                dist = np.sqrt(((sample_emb - centroid)**2).sum())\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    best_rad = radius\n",
        "\n",
        "            if min_dist > best_rad*self.dist_factor:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "            else:\n",
        "                out_11[i, 10] = -10.0\n",
        "\n",
        "        return out_11"
      ],
      "metadata": {
        "id": "s_cTRxgivrVX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((28,28)),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(0, translate=(0.1,0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((28,28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "mnist_train_full = MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
        "mnist_test       = MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "val_ratio = 0.1\n",
        "train_size = int((1 - val_ratio)*len(mnist_train_full))\n",
        "val_size   = len(mnist_train_full) - train_size\n",
        "mnist_train_ds, mnist_val_ds = random_split(mnist_train_full, [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(SEED)\n",
        ")\n",
        "mnist_val_ds.dataset.transform = transform_test\n",
        "\n",
        "train_loader = DataLoader(mnist_train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(mnist_val_ds,   batch_size=batch_size, shuffle=False)\n",
        "\n",
        "fashion_ds = FashionMNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "inds = list(range(len(fashion_ds)))\n",
        "random.shuffle(inds)\n",
        "inds_val  = inds[:num_ood_val]\n",
        "inds_test = inds[num_ood_val : num_ood_val + num_ood_test]\n",
        "\n",
        "fashion_ood_val_ds  = Subset(fashion_ds, inds_val)\n",
        "fashion_ood_test_ds = Subset(fashion_ds, inds_test)\n",
        "\n",
        "val_ood_ds  = CombinedDataset(mnist_val_ds,  fashion_ood_val_ds)\n",
        "test_ood_ds = CombinedDataset(mnist_test,    fashion_ood_test_ds)\n",
        "\n",
        "val_ood_loader  = DataLoader(val_ood_ds,  batch_size=256, shuffle=False)\n",
        "test_ood_loader = DataLoader(test_ood_ds, batch_size=256, shuffle=False)\n"
      ],
      "metadata": {
        "id": "cJ7Ck5dWZu0q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST, CIFAR10, EMNIST\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "batch_size = 128\n",
        "num_ood_val = 300   # e.g. number of CIFAR samples for OOD validation\n",
        "num_ood_test = 800  # e.g. number of CIFAR samples for OOD testing\n",
        "\n",
        "# ======================================================================\n",
        "#   CombinedDataset for OSR\n",
        "# ======================================================================\n",
        "class CombinedDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Merges two datasets: the first is 'in-dist' (label<10),\n",
        "    the second is OOD (label=10).\n",
        "    \"\"\"\n",
        "    def __init__(self, ds_in, ds_ood):\n",
        "        self.ds_in = ds_in\n",
        "        self.ds_ood = ds_ood\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds_in) + len(self.ds_ood)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < len(self.ds_in):\n",
        "            data, label = self.ds_in[idx]\n",
        "            return data, label  # MNIST label\n",
        "        else:\n",
        "            data, _ = self.ds_ood[idx - len(self.ds_in)]\n",
        "            return data, 10     # OOD label => 10\n",
        "\n",
        "# ======================================================================\n",
        "#   Transforms\n",
        "# ======================================================================\n",
        "# For MNIST:\n",
        "transform_mnist_train = transforms.Compose([\n",
        "    transforms.Resize((28,28)),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(0, translate=(0.1,0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "transform_mnist_test = transforms.Compose([\n",
        "    transforms.Resize((28,28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# For CIFAR10 -> to grayscale, resize 28x28, then normalize similarly\n",
        "transform_cifar = transforms.Compose([\n",
        "    transforms.Resize((28,28)),\n",
        "    transforms.Grayscale(num_output_channels=1),  # from RGB to 1 channel\n",
        "    transforms.ToTensor(),\n",
        "    # We can reuse the same mean/std as MNIST or recalc. We'll do the same for simplicity:\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# ======================================================================\n",
        "#   Loading MNIST as in-dist\n",
        "# ======================================================================\n",
        "mnist_train_full = MNIST(root='./data', train=True, download=True, transform=transform_mnist_train)\n",
        "mnist_test       = MNIST(root='./data', train=False, download=True, transform=transform_mnist_test)\n",
        "\n",
        "# We define a validation split from the training data\n",
        "val_ratio = 0.1\n",
        "train_size = int((1 - val_ratio)*len(mnist_train_full))\n",
        "val_size   = len(mnist_train_full) - train_size\n",
        "mnist_train_ds, mnist_val_ds = torch.utils.data.random_split(\n",
        "    mnist_train_full,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(SEED)\n",
        ")\n",
        "# override transform for val set:\n",
        "mnist_val_ds.dataset.transform = transform_mnist_test\n",
        "\n",
        "train_loader = DataLoader(mnist_train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(mnist_val_ds,   batch_size=batch_size, shuffle=False)\n",
        "mnist_test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ======================================================================\n",
        "#   Loading CIFAR10 as OOD\n",
        "# ======================================================================\n",
        "cifar_test = CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n",
        "# cifar_test = EMNIST(root=\"./data\",\n",
        "#                       split=\"balanced\",  # או 'byclass', 'bymerge', etc.\n",
        "#                       train=False,\n",
        "#                       download=True,\n",
        "#                       transform=transform_cifar)\n",
        "\n",
        "inds = list(range(len(cifar_test)))\n",
        "random.shuffle(inds)\n",
        "\n",
        "# separate: OOD for val, OOD for test\n",
        "inds_ood_val  = inds[:num_ood_val]\n",
        "inds_ood_test = inds[num_ood_val : num_ood_val + num_ood_test]\n",
        "\n",
        "cifar_ood_val_ds  = Subset(cifar_test, inds_ood_val)\n",
        "cifar_ood_test_ds = Subset(cifar_test, inds_ood_test)\n",
        "\n",
        "# Combined datasets for OSR\n",
        "val_ood_ds  = CombinedDataset(mnist_val_ds,  cifar_ood_val_ds)\n",
        "test_ood_ds = CombinedDataset(mnist_test,    cifar_ood_test_ds)\n",
        "\n",
        "val_ood_loader  = DataLoader(val_ood_ds,  batch_size=256, shuffle=False)\n",
        "test_ood_loader = DataLoader(test_ood_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "print(\"Datasets loaded. MNIST + CIFAR10 as OOD.\")\n",
        "fashion_ds = cifar_test\n",
        "fashion_ood_val_ds  = cifar_ood_val_ds\n",
        "fashion_ood_test_ds = cifar_ood_test_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN7PD-HE0RXp",
        "outputId": "ae6170c7-55c9-4149-c462-164541b3be2d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Files already downloaded and verified\n",
            "Datasets loaded. MNIST + CIFAR10 as OOD.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "core_model = CNN_EmbeddingNet_Triplet(embedding_dim=embedding_dim, dropout_p=0.3).to(device)\n",
        "optimizer = optim.Adam(core_model.parameters(), lr=lr)\n",
        "ce_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if not eval_mode:\n",
        "\n",
        "    print(\"\\n=== Computing DBSCAN clusters before training ===\")\n",
        "    dbscan_clusters = compute_dbscan_clusters(core_model, mnist_train_ds, eps=2.0, min_samples=5)\n",
        "\n",
        "    print(\"=== Training CNN Embedding Model (Triplet+CE) on MNIST ===\")\n",
        "    for ep in range(1, epochs+1):\n",
        "        total_loss, ce_val, trip_val, train_acc = train_one_epoch_triplet(\n",
        "            core_model, train_loader, optimizer, ce_criterion,\n",
        "            dbscan_clusters = dbscan_clusters,\n",
        "            triplet_margin=1.0, alpha=triplet_alpha\n",
        "\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_classifier(core_model, val_loader, ce_criterion)\n",
        "        print(f\"Epoch {ep}/{epochs} | \"\n",
        "              f\"TrainLoss={total_loss:.4f} (CE={ce_val:.4f}, T={trip_val:.4f}), \"\n",
        "              f\"Acc={train_acc:.4f} | ValLoss={val_loss:.4f}, ValAcc={val_acc:.4f}\")\n",
        "\n",
        "    torch.save(core_model.state_dict(), \"mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "    print(\"Saved weights to mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "else:\n",
        "    core_model.load_state_dict(torch.load(\"mnist_cnn_triplet_dbscan_tuning.pth\", map_location=device))\n",
        "    print(\"Loaded weights from mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "\n"
      ],
      "metadata": {
        "id": "m2apW4VXZyEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79b05053-f41b-4cce-acaf-f81ce8906132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Computing DBSCAN clusters before training ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eps_grid = [1.5, 2.0, 2.5]\n",
        "min_samples_grid = [3, 5]\n",
        "prob_grid = [0.6, 0.7, 0.8]\n",
        "dist_factor_grid = [1.0, 1.2]\n",
        "\n",
        "best_config = None\n",
        "best_acc = 0.0\n",
        "\n",
        "print(\"\\n=== Hyperparameter Tuning over DBSCAN + prob_threshold + dist_factor ===\")\n",
        "for eps_val in eps_grid:\n",
        "    for min_s in min_samples_grid:\n",
        "        # compute DBSCAN clusters for this combination\n",
        "        dbscan_dict = compute_dbscan_clusters(core_model, mnist_train_ds, eps=eps_val, min_samples=min_s)\n",
        "\n",
        "        for prob_t in prob_grid:\n",
        "            for dist_f in dist_factor_grid:\n",
        "                # build a wrapper model\n",
        "                temp_model = OSRWrapperDBSCAN(\n",
        "                    core_model,\n",
        "                    dbscan_dict,\n",
        "                    prob_threshold=prob_t,\n",
        "                    dist_factor=dist_f\n",
        "                ).to(device)\n",
        "\n",
        "                acc_mnist, acc_ood, acc_total = eval_model(temp_model, val_ood_loader, device)\n",
        "\n",
        "                if acc_total > best_acc:\n",
        "                    best_acc = acc_total\n",
        "                    best_config = (eps_val, min_s, prob_t, dist_f)\n",
        "                    print(f\"New best: eps={eps_val}, minS={min_s}, prob={prob_t}, distF={dist_f}, valAcc={acc_total*100:.2f}%\")\n",
        "\n",
        "eps_best, min_s_best, prob_best, dist_f_best = best_config\n",
        "print(f\"\\nBest config = eps={eps_best}, min_samples={min_s_best}, prob={prob_best}, dist_factor={dist_f_best}, valAcc={best_acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "RdIPtbCRWLBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  compute DBSCAN again with best config, and evaluate on test set\n",
        "dbscan_dict_final = compute_dbscan_clusters(core_model, mnist_train_ds, eps=eps_best, min_samples=min_s_best)\n",
        "final_model = OSRWrapperDBSCAN(core_model, dbscan_dict_final, prob_threshold=prob_best, dist_factor=dist_f_best).to(device)\n",
        "\n",
        "acc_mnist_test, acc_ood_test, acc_total_test = eval_model(final_model, test_ood_loader, device)\n",
        "print(\"\\n=== Final OSR (with best config) on Test ===\")\n",
        "print(f\"Accuracy on MNIST: {acc_mnist_test*100:.2f}%\")\n",
        "print(f\"Accuracy on OOD:   {acc_ood_test*100:.2f}%\")\n",
        "print(f\"Total Accuracy:    {acc_total_test*100:.2f}%\")\n",
        "\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "id": "1XYRFE-TwDpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
        "from torchvision.datasets import MNIST, CIFAR10, FashionMNIST\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "#%%\n",
        "# Basic Definitions and Setup\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "eval_mode = False       # Set to True for evaluation only\n",
        "epochs = 5              # Adjust as needed (should be under 10 minutes)\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "embedding_dim = 64\n",
        "triplet_alpha = 1.0     # Weight for Triplet Loss\n",
        "\n",
        "# Number of OOD samples for validation and testing\n",
        "num_ood_val = 300\n",
        "num_ood_test = 800\n",
        "\n",
        "#%%\n",
        "# CombinedDataset: Merges two datasets (in-dist and OOD).\n",
        "# OOD samples are assigned label 10.\n",
        "class CombinedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Merges two datasets (in-dist and OOD) where the OOD samples receive label 10.\n",
        "    \"\"\"\n",
        "    def __init__(self, ds_in, ds_ood):\n",
        "        self.ds_in = ds_in\n",
        "        self.ds_ood = ds_ood\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds_in) + len(self.ds_ood)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < len(self.ds_in):\n",
        "            data, label = self.ds_in[idx]\n",
        "            return data, label\n",
        "        else:\n",
        "            data, _ = self.ds_ood[idx - len(self.ds_in)]\n",
        "            return data, 10\n",
        "\n",
        "#%%\n",
        "def eval_model(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluates an OSR model that outputs (N, 11) logits (10 known classes + 1 'unknown').\n",
        "    Returns accuracy on in-dist (MNIST), OOD, and overall.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct_in = 0\n",
        "    total_in = 0\n",
        "    correct_ood = 0\n",
        "    total_ood = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in data_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)  # Expected shape (N, 11)\n",
        "            _, y_pred = torch.max(outputs, dim=1)\n",
        "\n",
        "            mask_in = (labels < 10)\n",
        "            mask_ood = (labels == 10)\n",
        "\n",
        "            labels_in = labels[mask_in]\n",
        "            labels_ood = labels[mask_ood]\n",
        "            pred_in = y_pred[mask_in]\n",
        "            pred_ood = y_pred[mask_ood]\n",
        "\n",
        "            total_in += labels_in.size(0)\n",
        "            correct_in += (pred_in == labels_in).sum().item()\n",
        "\n",
        "            total_ood += labels_ood.size(0)\n",
        "            correct_ood += (pred_ood == labels_ood).sum().item()\n",
        "\n",
        "    acc_in = correct_in / total_in if total_in > 0 else 0\n",
        "    acc_ood = correct_ood / total_ood if total_ood > 0 else 0\n",
        "    acc_total = (correct_in + correct_ood) / (total_in + total_ood)\n",
        "    return acc_in, acc_ood, acc_total\n",
        "\n",
        "#%%\n",
        "# CNN with Embedding Network and Triplet Loss\n",
        "class CNN_EmbeddingNet_Triplet(nn.Module):\n",
        "    def __init__(self, embedding_dim=64, dropout_p=0.3):\n",
        "        super().__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Fully connected layers for embedding\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc2 = nn.Linear(128, embedding_dim)\n",
        "\n",
        "        # Classifier for 10 known classes\n",
        "        self.classifier = nn.Linear(embedding_dim, 10)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        emb = self.fc2(x)\n",
        "        return emb\n",
        "\n",
        "    def forward_classifier(self, emb):\n",
        "        return self.classifier(emb)\n",
        "\n",
        "    def forward_inference_10(self, x):\n",
        "        emb = self.forward_features(x)\n",
        "        logits = self.forward_classifier(emb)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.forward_inference_10(x)\n",
        "\n",
        "#%%\n",
        "def train_one_epoch_triplet(model, loader, optimizer, ce_criterion, triplet_margin=1.0, alpha=1.0):\n",
        "    model.train()\n",
        "    triplet_criterion = nn.TripletMarginLoss(margin=triplet_margin, reduction='mean')\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_ce = 0.0\n",
        "    running_trip = 0.0\n",
        "    total_samples = 0\n",
        "    correct = 0\n",
        "\n",
        "    for data, labels in loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for classification\n",
        "        logits = model.forward_inference_10(data)\n",
        "        ce_loss = ce_criterion(logits, labels)\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        # Forward pass for embeddings\n",
        "        emb = model.forward_features(data)\n",
        "\n",
        "        # Create triplets for triplet loss\n",
        "        anchor_list, pos_list, neg_list = [], [], []\n",
        "        label2indices = {}\n",
        "        for i, lab in enumerate(labels):\n",
        "            lab_i = lab.item()\n",
        "            label2indices.setdefault(lab_i, []).append(i)\n",
        "\n",
        "        for i, lab in enumerate(labels):\n",
        "            anchor_label = lab.item()\n",
        "            if len(label2indices[anchor_label]) < 2:\n",
        "                continue\n",
        "            pos_idx = i\n",
        "            while pos_idx == i:\n",
        "                pos_idx = random.choice(label2indices[anchor_label])\n",
        "            neg_label = anchor_label\n",
        "            while neg_label == anchor_label:\n",
        "                neg_label = random.choice(list(label2indices.keys()))\n",
        "            neg_idx = random.choice(label2indices[neg_label])\n",
        "\n",
        "            anchor_list.append(emb[i].unsqueeze(0))\n",
        "            pos_list.append(emb[pos_idx].unsqueeze(0))\n",
        "            neg_list.append(emb[neg_idx].unsqueeze(0))\n",
        "\n",
        "        if len(anchor_list) > 0:\n",
        "            anchor_t = torch.cat(anchor_list, dim=0)\n",
        "            pos_t = torch.cat(pos_list, dim=0)\n",
        "            neg_t = torch.cat(neg_list, dim=0)\n",
        "            trip_loss = triplet_criterion(anchor_t, pos_t, neg_t)\n",
        "        else:\n",
        "            trip_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "        total_loss = ce_loss + alpha * trip_loss\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += total_loss.item() * data.size(0)\n",
        "        running_ce += ce_loss.item() * data.size(0)\n",
        "        running_trip += trip_loss.item() * data.size(0)\n",
        "\n",
        "    ep_loss = running_loss / total_samples\n",
        "    ep_ce = running_ce / total_samples\n",
        "    ep_trip = running_trip / total_samples\n",
        "    ep_acc = correct / total_samples\n",
        "    return ep_loss, ep_ce, ep_trip, ep_acc\n",
        "\n",
        "#%%\n",
        "def evaluate_classifier(model, loader, ce_criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels in loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            logits = model.forward_inference_10(data)\n",
        "            ce_loss = ce_criterion(logits, labels)\n",
        "            total_loss += ce_loss.item() * data.size(0)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "#%%\n",
        "def compute_dbscan_clusters(model, dataset, eps, min_samples):\n",
        "    \"\"\"\n",
        "    For each class (0..9), gather embeddings and run DBSCAN (with eps and min_samples).\n",
        "    Returns a dictionary mapping class -> list of clusters (each cluster has a centroid and a radius).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    label2vecs = {c: [] for c in range(10)}\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
        "    with torch.no_grad():\n",
        "        for data, labels in loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            emb = model.forward_features(data)\n",
        "            emb_np = emb.cpu().numpy()  # shape (B, embedding_dim)\n",
        "            for i, lab in enumerate(labels):\n",
        "                label2vecs[lab.item()].append(emb_np[i])\n",
        "\n",
        "    dbscan_dict = {}\n",
        "    for c in range(10):\n",
        "        arr = np.vstack(label2vecs[c])\n",
        "        if len(arr) == 0:\n",
        "            dbscan_dict[c] = []\n",
        "            continue\n",
        "        dbs = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        cluster_labels = dbs.fit_predict(arr)\n",
        "        clusters_info = []\n",
        "        unique_labels = set(cluster_labels) - {-1}\n",
        "        for clab in unique_labels:\n",
        "            points = arr[cluster_labels == clab]\n",
        "            centroid = points.mean(axis=0)\n",
        "            dists = np.sqrt(((points - centroid) ** 2).sum(axis=1))\n",
        "            radius = dists.max()\n",
        "            clusters_info.append({\"centroid\": centroid, \"radius\": radius})\n",
        "        dbscan_dict[c] = clusters_info\n",
        "    return dbscan_dict\n",
        "\n",
        "#%%\n",
        "class OSRWrapperDBSCAN(nn.Module):\n",
        "    def __init__(self, core_model, dbscan_dict, prob_threshold=0.7, dist_factor=1.0):\n",
        "        \"\"\"\n",
        "        1) If max probability < prob_threshold, classify as unknown.\n",
        "        2) Otherwise, for the predicted class, check the distance of the sample's embedding\n",
        "           to the nearest cluster. If the distance exceeds (radius * dist_factor), mark as unknown.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.core_model = core_model\n",
        "        self.dbscan_dict = dbscan_dict\n",
        "        self.prob_threshold = prob_threshold\n",
        "        self.dist_factor = dist_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.core_model.forward_features(x)\n",
        "        logits_10 = self.core_model.forward_classifier(emb)\n",
        "        probs_10 = F.softmax(logits_10, dim=1)\n",
        "\n",
        "        # Prepare output logits for 11 classes: copy known class logits and reserve slot 10 for unknown.\n",
        "        out_11 = torch.zeros(x.size(0), 11, device=x.device)\n",
        "        out_11[:, :10] = logits_10\n",
        "\n",
        "        max_probs, pred_class = torch.max(probs_10, dim=1)\n",
        "        emb_np = emb.detach().cpu().numpy()\n",
        "\n",
        "        for i in range(x.size(0)):\n",
        "            p = max_probs[i].item()\n",
        "            c = pred_class[i].item()\n",
        "            if p < self.prob_threshold:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "                continue\n",
        "\n",
        "            clusters_info = self.dbscan_dict[c]\n",
        "            if len(clusters_info) == 0:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "                continue\n",
        "\n",
        "            sample_emb = emb_np[i]\n",
        "            min_dist = 1e9\n",
        "            best_radius = 0.0\n",
        "            for info in clusters_info:\n",
        "                centroid = info[\"centroid\"]\n",
        "                radius = info[\"radius\"]\n",
        "                dist = np.sqrt(((sample_emb - centroid) ** 2).sum())\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    best_radius = radius\n",
        "            if min_dist > best_radius * self.dist_factor:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "            else:\n",
        "                out_11[i, 10] = -10.0\n",
        "\n",
        "        return out_11\n",
        "\n",
        "#%%\n",
        "# Data Transforms\n",
        "# For MNIST (in-distribution)\n",
        "transform_mnist_train = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "transform_mnist_test = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# For CIFAR10 (OOD): convert RGB to grayscale, resize, and normalize\n",
        "transform_cifar = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "#%%\n",
        "# Load MNIST as in-distribution data\n",
        "mnist_train_full = MNIST(root='./data', train=True, download=True, transform=transform_mnist_train)\n",
        "mnist_test = MNIST(root='./data', train=False, download=True, transform=transform_mnist_test)\n",
        "\n",
        "# Create a validation split from MNIST training data (90%-10%)\n",
        "val_ratio = 0.1\n",
        "train_size = int((1 - val_ratio) * len(mnist_train_full))\n",
        "val_size = len(mnist_train_full) - train_size\n",
        "mnist_train_ds, mnist_val_ds = random_split(\n",
        "    mnist_train_full,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(SEED)\n",
        ")\n",
        "mnist_val_ds.dataset.transform = transform_mnist_test\n",
        "\n",
        "train_loader = DataLoader(mnist_train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(mnist_val_ds, batch_size=batch_size, shuffle=False)\n",
        "mnist_test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#%%\n",
        "# Load CIFAR10 as OOD data using CIFAR transform\n",
        "cifar_test = CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n",
        "\n",
        "inds = list(range(len(cifar_test)))\n",
        "random.shuffle(inds)\n",
        "inds_ood_val = inds[:num_ood_val]\n",
        "inds_ood_test = inds[num_ood_val : num_ood_val + num_ood_test]\n",
        "\n",
        "cifar_ood_val_ds = Subset(cifar_test, inds_ood_val)\n",
        "cifar_ood_test_ds = Subset(cifar_test, inds_ood_test)\n",
        "\n",
        "# Create combined datasets for OSR evaluation (in-dist + OOD)\n",
        "val_ood_ds = CombinedDataset(mnist_val_ds, cifar_ood_val_ds)\n",
        "test_ood_ds = CombinedDataset(mnist_test, cifar_ood_test_ds)\n",
        "\n",
        "val_ood_loader = DataLoader(val_ood_ds, batch_size=256, shuffle=False)\n",
        "test_ood_loader = DataLoader(test_ood_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "print(\"Datasets loaded. MNIST + CIFAR10 as OOD.\")\n",
        "\n",
        "#%%\n",
        "# Initialize core model and optimizer\n",
        "core_model = CNN_EmbeddingNet_Triplet(embedding_dim=embedding_dim, dropout_p=0.3).to(device)\n",
        "optimizer = optim.Adam(core_model.parameters(), lr=lr)\n",
        "ce_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Lists to store training and validation losses for plotting\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "if not eval_mode:\n",
        "\n",
        "    print(\"\\n=== Computing DBSCAN clusters before training ===\")\n",
        "    dbscan_clusters = compute_dbscan_clusters(core_model, mnist_train_ds, eps=2.0, min_samples=5)\n",
        "\n",
        "    print(\"=== Training CNN Embedding Model (Triplet + CE) on MNIST ===\")\n",
        "    for ep in range(1, epochs + 1):\n",
        "        total_loss, ce_loss_val, trip_loss_val, train_acc = train_one_epoch_triplet(\n",
        "            core_model, train_loader, optimizer, ce_criterion,\n",
        "            dbscan_clusters = dbscan_clusters,\n",
        "            triplet_margin=1.0, alpha=triplet_alpha\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_classifier(core_model, val_loader, ce_criterion)\n",
        "        train_losses.append(total_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f\"Epoch {ep}/{epochs} | TrainLoss={total_loss:.4f} (CE={ce_loss_val:.4f}, T={trip_loss_val:.4f}), Acc={train_acc:.4f} | ValLoss={val_loss:.4f}, ValAcc={val_acc:.4f}\")\n",
        "    torch.save(core_model.state_dict(), \"mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "    print(\"Saved weights to mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "else:\n",
        "    core_model.load_state_dict(torch.load(\"mnist_cnn_triplet_dbscan_tuning.pth\", map_location=device))\n",
        "    print(\"Loaded weights from mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "\n",
        "#%%\n",
        "# Plot Training and Validation Losses (if training was performed)\n",
        "if not eval_mode:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
        "    plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "#%%\n",
        "# Hyperparameter tuning for DBSCAN + probability threshold + distance factor\n",
        "eps_grid = [1.5, 2.0, 2.5]\n",
        "min_samples_grid = [3, 5]\n",
        "prob_grid = [0.6, 0.7, 0.8]\n",
        "dist_factor_grid = [1.0, 1.2]\n",
        "\n",
        "best_config = None\n",
        "best_acc = 0.0\n",
        "\n",
        "print(\"\\n=== Hyperparameter Tuning over DBSCAN + prob_threshold + dist_factor ===\")\n",
        "for eps_val in eps_grid:\n",
        "    for min_s in min_samples_grid:\n",
        "        # Compute DBSCAN clusters for this combination using the MNIST training set\n",
        "        dbscan_dict = compute_dbscan_clusters(core_model, mnist_train_ds, eps=eps_val, min_samples=min_s)\n",
        "        for prob_t in prob_grid:\n",
        "            for dist_f in dist_factor_grid:\n",
        "                temp_model = OSRWrapperDBSCAN(\n",
        "                    core_model,\n",
        "                    dbscan_dict,\n",
        "                    prob_threshold=prob_t,\n",
        "                    dist_factor=dist_f\n",
        "                ).to(device)\n",
        "                acc_in, acc_ood, acc_total = eval_model(temp_model, val_ood_loader, device)\n",
        "                if acc_total > best_acc:\n",
        "                    best_acc = acc_total\n",
        "                    best_config = (eps_val, min_s, prob_t, dist_f)\n",
        "                    print(f\"New best: eps={eps_val}, minS={min_s}, prob={prob_t}, distF={dist_f}, valAcc={acc_total*100:.2f}%\")\n",
        "\n",
        "eps_best, min_s_best, prob_best, dist_f_best = best_config\n",
        "print(f\"\\nBest config = eps={eps_best}, min_samples={min_s_best}, prob={prob_best}, dist_factor={dist_f_best}, valAcc={best_acc*100:.2f}%\")\n",
        "\n",
        "#%%\n",
        "# Compute final DBSCAN clusters with the best config and evaluate on the test set\n",
        "dbscan_dict_final = compute_dbscan_clusters(core_model, mnist_train_ds, eps=eps_best, min_samples=min_s_best)\n",
        "final_model = OSRWrapperDBSCAN(core_model, dbscan_dict_final, prob_threshold=prob_best, dist_factor=dist_f_best).to(device)\n",
        "\n",
        "acc_in_test, acc_ood_test, acc_total_test = eval_model(final_model, test_ood_loader, device)\n",
        "print(\"\\n=== Final OSR (with best config) on Test Set ===\")\n",
        "print(f\"Accuracy on MNIST (in-dist): {acc_in_test*100:.2f}%\")\n",
        "print(f\"Accuracy on OOD:            {acc_ood_test*100:.2f}%\")\n",
        "print(f\"Overall Accuracy:           {acc_total_test*100:.2f}%\")\n",
        "\n",
        "#%%\n",
        "# Additional Plots for Analysis\n",
        "# 1. Confusion Matrix for 11 classes (0-9 + Unknown)\n",
        "final_model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_ood_loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        outputs = final_model(data)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "all_preds = np.concatenate(all_preds)\n",
        "all_labels = np.concatenate(all_labels)\n",
        "\n",
        "cm_11 = confusion_matrix(all_labels, all_preds, labels=list(range(10)) + [10])\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_11, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=list(range(10)) + ['Unknown'],\n",
        "            yticklabels=list(range(10)) + ['Unknown'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (11 Classes)')\n",
        "plt.show()\n",
        "\n",
        "# 2. Binary Confusion Matrix (Known vs Unknown)\n",
        "binary_preds = np.where(all_preds < 10, 0, 1)\n",
        "binary_labels = np.where(all_labels < 10, 0, 1)\n",
        "cm_bin = confusion_matrix(binary_labels, binary_preds, labels=[0, 1])\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_bin, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=['Known', 'Unknown'],\n",
        "            yticklabels=['Known', 'Unknown'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Binary Confusion Matrix (Known vs Unknown)')\n",
        "plt.show()\n",
        "\n",
        "# 3. t-SNE Visualization of Embeddings from Test Set\n",
        "all_embeddings = []\n",
        "all_labels_emb = []\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_ood_loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        emb = final_model.core_model.forward_features(data)\n",
        "        all_embeddings.append(emb.cpu().numpy())\n",
        "        all_labels_emb.append(labels.cpu().numpy())\n",
        "all_embeddings = np.concatenate(all_embeddings)\n",
        "all_labels_emb = np.concatenate(all_labels_emb)\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "embeddings_2d = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
        "colors = np.vstack([colors, np.array([0.5, 0.5, 0.5, 1])])  # Color for Unknown\n",
        "for label in np.unique(all_labels_emb):\n",
        "    idx = all_labels_emb == label\n",
        "    plt.scatter(embeddings_2d[idx, 0], embeddings_2d[idx, 1],\n",
        "                color=colors[label] if label < 10 else colors[-1],\n",
        "                label=str(label) if label < 10 else 'Unknown',\n",
        "                alpha=0.6)\n",
        "plt.legend()\n",
        "plt.title('t-SNE Visualization of Embeddings')\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.show()\n",
        "\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "id": "7NiJrTXFQ5OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
        "from torchvision.datasets import MNIST, CIFAR10, FashionMNIST\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "#%%\n",
        "# Basic Definitions and Setup\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "eval_mode = False       # Set to True for evaluation only\n",
        "epochs = 5              # Adjust as needed (should be under 10 minutes)\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "embedding_dim = 64\n",
        "triplet_alpha = 1.0     # Weight for Triplet Loss\n",
        "\n",
        "# Number of OOD samples for validation and testing\n",
        "num_ood_val = 300\n",
        "num_ood_test = 800\n",
        "\n",
        "#%%\n",
        "# CombinedDataset: Merges two datasets (in-dist and OOD).\n",
        "# OOD samples are assigned label 10.\n",
        "class CombinedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Merges two datasets (in-dist and OOD) where the OOD samples receive label 10.\n",
        "    \"\"\"\n",
        "    def __init__(self, ds_in, ds_ood):\n",
        "        self.ds_in = ds_in\n",
        "        self.ds_ood = ds_ood\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds_in) + len(self.ds_ood)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < len(self.ds_in):\n",
        "            data, label = self.ds_in[idx]\n",
        "            return data, label\n",
        "        else:\n",
        "            data, _ = self.ds_ood[idx - len(self.ds_in)]\n",
        "            return data, 10\n",
        "\n",
        "#%%\n",
        "def eval_model(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluates an OSR model that outputs (N, 11) logits (10 known classes + 1 'unknown').\n",
        "    Returns accuracy on in-dist (MNIST), OOD, and overall.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct_in = 0\n",
        "    total_in = 0\n",
        "    correct_ood = 0\n",
        "    total_ood = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in data_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)  # Expected shape (N, 11)\n",
        "            _, y_pred = torch.max(outputs, dim=1)\n",
        "\n",
        "            mask_in = (labels < 10)\n",
        "            mask_ood = (labels == 10)\n",
        "\n",
        "            labels_in = labels[mask_in]\n",
        "            labels_ood = labels[mask_ood]\n",
        "            pred_in = y_pred[mask_in]\n",
        "            pred_ood = y_pred[mask_ood]\n",
        "\n",
        "            total_in += labels_in.size(0)\n",
        "            correct_in += (pred_in == labels_in).sum().item()\n",
        "\n",
        "            total_ood += labels_ood.size(0)\n",
        "            correct_ood += (pred_ood == labels_ood).sum().item()\n",
        "\n",
        "    acc_in = correct_in / total_in if total_in > 0 else 0\n",
        "    acc_ood = correct_ood / total_ood if total_ood > 0 else 0\n",
        "    acc_total = (correct_in + correct_ood) / (total_in + total_ood)\n",
        "    return acc_in, acc_ood, acc_total\n",
        "\n",
        "#%%\n",
        "# CNN with Embedding Network and Triplet Loss\n",
        "class CNN_EmbeddingNet_Triplet(nn.Module):\n",
        "    def __init__(self, embedding_dim=64, dropout_p=0.3):\n",
        "        super().__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Fully connected layers for embedding\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc2 = nn.Linear(128, embedding_dim)\n",
        "\n",
        "        # Classifier for 10 known classes\n",
        "        self.classifier = nn.Linear(embedding_dim, 10)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        emb = self.fc2(x)\n",
        "        return emb\n",
        "\n",
        "    def forward_classifier(self, emb):\n",
        "        return self.classifier(emb)\n",
        "\n",
        "    def forward_inference_10(self, x):\n",
        "        emb = self.forward_features(x)\n",
        "        logits = self.forward_classifier(emb)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.forward_inference_10(x)\n",
        "\n",
        "#%%\n",
        "def train_one_epoch_triplet(model, loader, optimizer, ce_criterion, triplet_margin=1.0, alpha=1.0):\n",
        "    model.train()\n",
        "    triplet_criterion = nn.TripletMarginLoss(margin=triplet_margin, reduction='mean')\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_ce = 0.0\n",
        "    running_trip = 0.0\n",
        "    total_samples = 0\n",
        "    correct = 0\n",
        "\n",
        "    for data, labels in loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for classification\n",
        "        logits = model.forward_inference_10(data)\n",
        "        ce_loss = ce_criterion(logits, labels)\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        # Forward pass for embeddings\n",
        "        emb = model.forward_features(data)\n",
        "\n",
        "        # Create triplets for triplet loss\n",
        "        anchor_list, pos_list, neg_list = [], [], []\n",
        "        label2indices = {}\n",
        "        for i, lab in enumerate(labels):\n",
        "            lab_i = lab.item()\n",
        "            label2indices.setdefault(lab_i, []).append(i)\n",
        "\n",
        "        for i, lab in enumerate(labels):\n",
        "            anchor_label = lab.item()\n",
        "            if len(label2indices[anchor_label]) < 2:\n",
        "                continue\n",
        "            pos_idx = i\n",
        "            while pos_idx == i:\n",
        "                pos_idx = random.choice(label2indices[anchor_label])\n",
        "            neg_label = anchor_label\n",
        "            while neg_label == anchor_label:\n",
        "                neg_label = random.choice(list(label2indices.keys()))\n",
        "            neg_idx = random.choice(label2indices[neg_label])\n",
        "\n",
        "            anchor_list.append(emb[i].unsqueeze(0))\n",
        "            pos_list.append(emb[pos_idx].unsqueeze(0))\n",
        "            neg_list.append(emb[neg_idx].unsqueeze(0))\n",
        "\n",
        "        if len(anchor_list) > 0:\n",
        "            anchor_t = torch.cat(anchor_list, dim=0)\n",
        "            pos_t = torch.cat(pos_list, dim=0)\n",
        "            neg_t = torch.cat(neg_list, dim=0)\n",
        "            trip_loss = triplet_criterion(anchor_t, pos_t, neg_t)\n",
        "        else:\n",
        "            trip_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "        total_loss = ce_loss + alpha * trip_loss\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += total_loss.item() * data.size(0)\n",
        "        running_ce += ce_loss.item() * data.size(0)\n",
        "        running_trip += trip_loss.item() * data.size(0)\n",
        "\n",
        "    ep_loss = running_loss / total_samples\n",
        "    ep_ce = running_ce / total_samples\n",
        "    ep_trip = running_trip / total_samples\n",
        "    ep_acc = correct / total_samples\n",
        "    return ep_loss, ep_ce, ep_trip, ep_acc\n",
        "\n",
        "#%%\n",
        "def evaluate_classifier(model, loader, ce_criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels in loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            logits = model.forward_inference_10(data)\n",
        "            ce_loss = ce_criterion(logits, labels)\n",
        "            total_loss += ce_loss.item() * data.size(0)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "#%%\n",
        "def compute_dbscan_clusters(model, dataset, eps, min_samples):\n",
        "    \"\"\"\n",
        "    For each class (0..9), gather embeddings and run DBSCAN (with eps and min_samples).\n",
        "    Returns a dictionary mapping class -> list of clusters (each cluster has a centroid and a radius).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    label2vecs = {c: [] for c in range(10)}\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
        "    with torch.no_grad():\n",
        "        for data, labels in loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            emb = model.forward_features(data)\n",
        "            emb_np = emb.cpu().numpy()  # shape (B, embedding_dim)\n",
        "            for i, lab in enumerate(labels):\n",
        "                label2vecs[lab.item()].append(emb_np[i])\n",
        "\n",
        "    dbscan_dict = {}\n",
        "    for c in range(10):\n",
        "        arr = np.vstack(label2vecs[c])\n",
        "        if len(arr) == 0:\n",
        "            dbscan_dict[c] = []\n",
        "            continue\n",
        "        dbs = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        cluster_labels = dbs.fit_predict(arr)\n",
        "        clusters_info = []\n",
        "        unique_labels = set(cluster_labels) - {-1}\n",
        "        for clab in unique_labels:\n",
        "            points = arr[cluster_labels == clab]\n",
        "            centroid = points.mean(axis=0)\n",
        "            dists = np.sqrt(((points - centroid) ** 2).sum(axis=1))\n",
        "            radius = dists.max()\n",
        "            clusters_info.append({\"centroid\": centroid, \"radius\": radius})\n",
        "        dbscan_dict[c] = clusters_info\n",
        "    return dbscan_dict\n",
        "\n",
        "#%%\n",
        "class OSRWrapperDBSCAN(nn.Module):\n",
        "    def __init__(self, core_model, dbscan_dict, prob_threshold=0.7, dist_factor=1.0):\n",
        "        \"\"\"\n",
        "        1) If max probability < prob_threshold, classify as unknown.\n",
        "        2) Otherwise, for the predicted class, check the distance of the sample's embedding\n",
        "           to the nearest cluster. If the distance exceeds (radius * dist_factor), mark as unknown.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.core_model = core_model\n",
        "        self.dbscan_dict = dbscan_dict\n",
        "        self.prob_threshold = prob_threshold\n",
        "        self.dist_factor = dist_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.core_model.forward_features(x)\n",
        "        logits_10 = self.core_model.forward_classifier(emb)\n",
        "        probs_10 = F.softmax(logits_10, dim=1)\n",
        "\n",
        "        # Prepare output logits for 11 classes: copy known class logits and reserve slot 10 for unknown.\n",
        "        out_11 = torch.zeros(x.size(0), 11, device=x.device)\n",
        "        out_11[:, :10] = logits_10\n",
        "\n",
        "        max_probs, pred_class = torch.max(probs_10, dim=1)\n",
        "        emb_np = emb.detach().cpu().numpy()\n",
        "\n",
        "        for i in range(x.size(0)):\n",
        "            p = max_probs[i].item()\n",
        "            c = pred_class[i].item()\n",
        "            if p < self.prob_threshold:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "                continue\n",
        "\n",
        "            clusters_info = self.dbscan_dict[c]\n",
        "            if len(clusters_info) == 0:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "                continue\n",
        "\n",
        "            sample_emb = emb_np[i]\n",
        "            min_dist = 1e9\n",
        "            best_radius = 0.0\n",
        "            for info in clusters_info:\n",
        "                centroid = info[\"centroid\"]\n",
        "                radius = info[\"radius\"]\n",
        "                dist = np.sqrt(((sample_emb - centroid) ** 2).sum())\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    best_radius = radius\n",
        "            if min_dist > best_radius * self.dist_factor:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "            else:\n",
        "                out_11[i, 10] = -10.0\n",
        "\n",
        "        return out_11\n",
        "\n",
        "#%%\n",
        "# Data Transforms\n",
        "# For MNIST (in-distribution)\n",
        "transform_mnist_train = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "transform_mnist_test = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# For CIFAR10 (OOD): convert RGB to grayscale, resize, and normalize\n",
        "transform_cifar = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "#%%\n",
        "# Load MNIST as in-distribution data\n",
        "mnist_train_full = MNIST(root='./data', train=True, download=True, transform=transform_mnist_train)\n",
        "mnist_test = MNIST(root='./data', train=False, download=True, transform=transform_mnist_test)\n",
        "\n",
        "# Create a validation split from MNIST training data (90%-10%)\n",
        "val_ratio = 0.1\n",
        "train_size = int((1 - val_ratio) * len(mnist_train_full))\n",
        "val_size = len(mnist_train_full) - train_size\n",
        "mnist_train_ds, mnist_val_ds = random_split(\n",
        "    mnist_train_full,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(SEED)\n",
        ")\n",
        "mnist_val_ds.dataset.transform = transform_mnist_test\n",
        "\n",
        "train_loader = DataLoader(mnist_train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(mnist_val_ds, batch_size=batch_size, shuffle=False)\n",
        "mnist_test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#%%\n",
        "# Load CIFAR10 as OOD data using CIFAR transform\n",
        "# cifar_test = CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n",
        "cifar_test = FashionMNIST(root='./data', train=False, download=True, transform=transform_cifar)\n",
        "\n",
        "inds = list(range(len(cifar_test)))\n",
        "random.shuffle(inds)\n",
        "inds_ood_val = inds[:num_ood_val]\n",
        "inds_ood_test = inds[num_ood_val : num_ood_val + num_ood_test]\n",
        "\n",
        "cifar_ood_val_ds = Subset(cifar_test, inds_ood_val)\n",
        "cifar_ood_test_ds = Subset(cifar_test, inds_ood_test)\n",
        "\n",
        "# Create combined datasets for OSR evaluation (in-dist + OOD)\n",
        "val_ood_ds = CombinedDataset(mnist_val_ds, cifar_ood_val_ds)\n",
        "test_ood_ds = CombinedDataset(mnist_test, cifar_ood_test_ds)\n",
        "\n",
        "val_ood_loader = DataLoader(val_ood_ds, batch_size=256, shuffle=False)\n",
        "test_ood_loader = DataLoader(test_ood_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "print(\"Datasets loaded. MNIST + CIFAR10 as OOD.\")\n",
        "\n",
        "#%%\n",
        "# Initialize core model and optimizer\n",
        "core_model = CNN_EmbeddingNet_Triplet(embedding_dim=embedding_dim, dropout_p=0.3).to(device)\n",
        "optimizer = optim.Adam(core_model.parameters(), lr=lr)\n",
        "ce_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Lists to store training and validation losses for plotting\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "if not eval_mode:\n",
        "\n",
        "    print(\"\\n=== Computing DBSCAN clusters before training ===\")\n",
        "    dbscan_clusters = compute_dbscan_clusters(core_model, mnist_train_ds, eps=2.0, min_samples=5)\n",
        "\n",
        "    print(\"=== Training CNN Embedding Model (Triplet + CE) on MNIST ===\")\n",
        "    for ep in range(1, epochs + 1):\n",
        "        total_loss, ce_loss_val, trip_loss_val, train_acc = train_one_epoch_triplet(\n",
        "            core_model, train_loader, optimizer, ce_criterion,\n",
        "            dbscan_clusters = dbscan_clusters,\n",
        "            triplet_margin=1.0, alpha=triplet_alpha\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_classifier(core_model, val_loader, ce_criterion)\n",
        "        train_losses.append(total_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f\"Epoch {ep}/{epochs} | TrainLoss={total_loss:.4f} (CE={ce_loss_val:.4f}, T={trip_loss_val:.4f}), Acc={train_acc:.4f} | ValLoss={val_loss:.4f}, ValAcc={val_acc:.4f}\")\n",
        "    torch.save(core_model.state_dict(), \"mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "    print(\"Saved weights to mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "else:\n",
        "    core_model.load_state_dict(torch.load(\"mnist_cnn_triplet_dbscan_tuning.pth\", map_location=device))\n",
        "    print(\"Loaded weights from mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "\n",
        "#%%\n",
        "# Plot Training and Validation Losses (if training was performed)\n",
        "if not eval_mode:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
        "    plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "#%%\n",
        "# Hyperparameter tuning for DBSCAN + probability threshold + distance factor\n",
        "eps_grid = [1.5, 2.0, 2.5]\n",
        "min_samples_grid = [3, 5]\n",
        "prob_grid = [0.6, 0.7, 0.8]\n",
        "dist_factor_grid = [1.0, 1.2]\n",
        "\n",
        "best_config = None\n",
        "best_acc = 0.0\n",
        "\n",
        "print(\"\\n=== Hyperparameter Tuning over DBSCAN + prob_threshold + dist_factor ===\")\n",
        "for eps_val in eps_grid:\n",
        "    for min_s in min_samples_grid:\n",
        "        # Compute DBSCAN clusters for this combination using the MNIST training set\n",
        "        dbscan_dict = compute_dbscan_clusters(core_model, mnist_train_ds, eps=eps_val, min_samples=min_s)\n",
        "        for prob_t in prob_grid:\n",
        "            for dist_f in dist_factor_grid:\n",
        "                temp_model = OSRWrapperDBSCAN(\n",
        "                    core_model,\n",
        "                    dbscan_dict,\n",
        "                    prob_threshold=prob_t,\n",
        "                    dist_factor=dist_f\n",
        "                ).to(device)\n",
        "                acc_in, acc_ood, acc_total = eval_model(temp_model, val_ood_loader, device)\n",
        "                if acc_total > best_acc:\n",
        "                    best_acc = acc_total\n",
        "                    best_config = (eps_val, min_s, prob_t, dist_f)\n",
        "                    print(f\"New best: eps={eps_val}, minS={min_s}, prob={prob_t}, distF={dist_f}, valAcc={acc_total*100:.2f}%\")\n",
        "\n",
        "eps_best, min_s_best, prob_best, dist_f_best = best_config\n",
        "print(f\"\\nBest config = eps={eps_best}, min_samples={min_s_best}, prob={prob_best}, dist_factor={dist_f_best}, valAcc={best_acc*100:.2f}%\")\n",
        "\n",
        "#%%\n",
        "# Compute final DBSCAN clusters with the best config and evaluate on the test set\n",
        "dbscan_dict_final = compute_dbscan_clusters(core_model, mnist_train_ds, eps=eps_best, min_samples=min_s_best)\n",
        "final_model = OSRWrapperDBSCAN(core_model, dbscan_dict_final, prob_threshold=prob_best, dist_factor=dist_f_best).to(device)\n",
        "\n",
        "acc_in_test, acc_ood_test, acc_total_test = eval_model(final_model, test_ood_loader, device)\n",
        "print(\"\\n=== Final OSR (with best config) on Test Set ===\")\n",
        "print(f\"Accuracy on MNIST (in-dist): {acc_in_test*100:.2f}%\")\n",
        "print(f\"Accuracy on OOD:            {acc_ood_test*100:.2f}%\")\n",
        "print(f\"Overall Accuracy:           {acc_total_test*100:.2f}%\")\n",
        "\n",
        "#%%\n",
        "# Additional Plots for Analysis\n",
        "# 1. Confusion Matrix for 11 classes (0-9 + Unknown)\n",
        "final_model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_ood_loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        outputs = final_model(data)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "all_preds = np.concatenate(all_preds)\n",
        "all_labels = np.concatenate(all_labels)\n",
        "\n",
        "cm_11 = confusion_matrix(all_labels, all_preds, labels=list(range(10)) + [10])\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_11, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=list(range(10)) + ['Unknown'],\n",
        "            yticklabels=list(range(10)) + ['Unknown'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (11 Classes)')\n",
        "plt.show()\n",
        "\n",
        "# 2. Binary Confusion Matrix (Known vs Unknown)\n",
        "binary_preds = np.where(all_preds < 10, 0, 1)\n",
        "binary_labels = np.where(all_labels < 10, 0, 1)\n",
        "cm_bin = confusion_matrix(binary_labels, binary_preds, labels=[0, 1])\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_bin, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=['Known', 'Unknown'],\n",
        "            yticklabels=['Known', 'Unknown'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Binary Confusion Matrix (Known vs Unknown)')\n",
        "plt.show()\n",
        "\n",
        "# 3. t-SNE Visualization of Embeddings from Test Set\n",
        "all_embeddings = []\n",
        "all_labels_emb = []\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_ood_loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        emb = final_model.core_model.forward_features(data)\n",
        "        all_embeddings.append(emb.cpu().numpy())\n",
        "        all_labels_emb.append(labels.cpu().numpy())\n",
        "all_embeddings = np.concatenate(all_embeddings)\n",
        "all_labels_emb = np.concatenate(all_labels_emb)\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "embeddings_2d = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
        "colors = np.vstack([colors, np.array([0.5, 0.5, 0.5, 1])])  # Color for Unknown\n",
        "for label in np.unique(all_labels_emb):\n",
        "    idx = all_labels_emb == label\n",
        "    plt.scatter(embeddings_2d[idx, 0], embeddings_2d[idx, 1],\n",
        "                color=colors[label] if label < 10 else colors[-1],\n",
        "                label=str(label) if label < 10 else 'Unknown',\n",
        "                alpha=0.6)\n",
        "plt.legend()\n",
        "plt.title('t-SNE Visualization of Embeddings')\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.show()\n",
        "\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "id": "f8e1R33eSyrI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}