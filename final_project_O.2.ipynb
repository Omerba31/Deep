{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Omerba31/Deep/blob/main/final_project_O.2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Omer Ben Arie:** &emsp; 211602842\n",
        "\n",
        "**Lee Ben Gigi:** &emsp;\n",
        "\n",
        "**Ron Gurevich:** &emsp; 318772456"
      ],
      "metadata": {
        "id": "JsQrQ0Gb-ytj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***imports***"
      ],
      "metadata": {
        "id": "OxD7EpUX__Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
        "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10, EMNIST\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from torch.utils.data import ConcatDataset"
      ],
      "metadata": {
        "id": "FHaWSKkKY9kJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Configuration and Environment Setup***"
      ],
      "metadata": {
        "id": "1XW1ao2MAinu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "eval_mode = False\n",
        "# eval_mode = True\n",
        "\n",
        "epochs = 5\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "embedding_dim = 64\n",
        "triplet_alpha = 1.0  # Weight for Triplet Loss\n",
        "\n",
        "batch_size = 128\n",
        "num_ood_val = 600\n",
        "num_ood_test = 1000"
      ],
      "metadata": {
        "id": "QVqL7KPMZIlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c152129-64fd-437e-b1b9-21337d7a196d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Datasets Combination***\n",
        "&emsp;&emsp; (ood_label = 10)"
      ],
      "metadata": {
        "id": "jXKHMwqMArcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CombineDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds_in, ds_ood):\n",
        "        self.ds_in = ds_in\n",
        "        self.ds_ood = ds_ood\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds_in) + len(self.ds_ood)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < len(self.ds_in):\n",
        "            data, label = self.ds_in[idx]\n",
        "            return data, label\n",
        "        else:\n",
        "            data, _ = self.ds_ood[idx - len(self.ds_in)]\n",
        "            return data, 10"
      ],
      "metadata": {
        "id": "Ab9Y6HYVZVZP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Evaluation Function***"
      ],
      "metadata": {
        "id": "rHG7-2fMCG3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, data_loader, device):\n",
        "\n",
        "    model.eval()\n",
        "    correct_mnist = 0\n",
        "    total_mnist = 0\n",
        "    correct_ood = 0\n",
        "    total_ood = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in data_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)  # shape (N,11)\n",
        "            _, y_pred = torch.max(outputs, dim=1)\n",
        "\n",
        "            mask_in = (labels < 10) # MNIST\n",
        "            mask_ood = (labels == 10) # OOD\n",
        "\n",
        "            labels_in = labels[mask_in]\n",
        "            labels_ood = labels[mask_ood]\n",
        "\n",
        "            pred_in = y_pred[mask_in]\n",
        "            pred_ood = y_pred[mask_ood]\n",
        "\n",
        "            total_mnist += labels_in.size(0)\n",
        "            correct_mnist += (pred_in == labels_in).sum().item()\n",
        "\n",
        "            total_ood += labels_ood.size(0)\n",
        "            correct_ood += (pred_ood == labels_ood).sum().item()\n",
        "\n",
        "    acc_mnist = correct_mnist / total_mnist if total_mnist>0 else 0 # mnist Accuracy\n",
        "    acc_ood   = correct_ood / total_ood     if total_ood>0   else 0 # ood Accuracy\n",
        "    acc_total = (correct_mnist + correct_ood) / (total_mnist + total_ood) # overall Accuracy\n",
        "\n",
        "    return acc_mnist, acc_ood, acc_total\n"
      ],
      "metadata": {
        "id": "-g4x7uq6ZXtw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Net initialization***"
      ],
      "metadata": {
        "id": "Gweo_aLAHqFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_EmbeddingNet_Triplet(nn.Module):\n",
        "    def __init__(self, embedding_dim=64, dropout_p=0.3):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        # Convs\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "        # FC\n",
        "        self.fc1 = nn.Linear(64*7*7, 128)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc2 = nn.Linear(128, embedding_dim)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(embedding_dim, 10)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        emb = self.fc2(x)\n",
        "        return emb\n",
        "\n",
        "    def forward_classifier(self, emb):\n",
        "        return self.classifier(emb)\n",
        "\n",
        "    def forward_inference_10(self, x):\n",
        "        emb = self.forward_features(x)\n",
        "        logits = self.forward_classifier(emb)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.forward_inference_10(x)\n"
      ],
      "metadata": {
        "id": "7WtnpjA-dVvC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Train***"
      ],
      "metadata": {
        "id": "u6XTrBeeH_6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch_triplet(model, loader, optimizer, ce_criterion, triplet_margin=1.0, alpha=1.0):\n",
        "    model.train()\n",
        "    triplet_criterion = nn.TripletMarginLoss(margin=triplet_margin, reduction='mean')\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_ce   = 0.0\n",
        "    running_trip = 0.0\n",
        "    total_samples = 0\n",
        "    correct = 0\n",
        "\n",
        "    for data, labels in loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model.forward_inference_10(data)\n",
        "        ce_loss = ce_criterion(logits, labels)\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        emb = model.forward_features(data)\n",
        "\n",
        "        anchor_list = []\n",
        "        pos_list = []\n",
        "        neg_list = []\n",
        "\n",
        "        label2indices = {}\n",
        "        for i, lab in enumerate(labels):\n",
        "            lab_i = lab.item()\n",
        "            label2indices.setdefault(lab_i, []).append(i)\n",
        "\n",
        "        for i, lab in enumerate(labels):\n",
        "            anchor_label = lab.item()\n",
        "            if len(label2indices[anchor_label]) < 2:\n",
        "                continue\n",
        "            pos_idx = i\n",
        "            while pos_idx == i:\n",
        "                pos_idx = random.choice(label2indices[anchor_label])\n",
        "\n",
        "            neg_label = anchor_label\n",
        "            while neg_label == anchor_label:\n",
        "                neg_label = random.choice(list(label2indices.keys()))\n",
        "            neg_idx = random.choice(label2indices[neg_label])\n",
        "\n",
        "            anchor_list.append(emb[i].unsqueeze(0))\n",
        "            pos_list.append(emb[pos_idx].unsqueeze(0))\n",
        "            neg_list.append(emb[neg_idx].unsqueeze(0))\n",
        "\n",
        "        if len(anchor_list) > 0:\n",
        "            anchor_t = torch.cat(anchor_list, dim=0)\n",
        "            pos_t = torch.cat(pos_list, dim=0)\n",
        "            neg_t = torch.cat(neg_list, dim=0)\n",
        "            trip_loss = triplet_criterion(anchor_t, pos_t, neg_t)\n",
        "        else:\n",
        "            trip_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "        total_loss = ce_loss + alpha*trip_loss\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += total_loss.item() * data.size(0)\n",
        "        running_ce   += ce_loss.item() * data.size(0)\n",
        "        running_trip += trip_loss.item() * data.size(0)\n",
        "\n",
        "    ep_loss = running_loss / total_samples\n",
        "    ep_ce   = running_ce / total_samples\n",
        "    ep_trip = running_trip / total_samples\n",
        "    ep_acc  = correct / total_samples\n",
        "\n",
        "    return ep_loss, ep_ce, ep_trip, ep_acc"
      ],
      "metadata": {
        "id": "nHkGcoaIZhAH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Classifier Evaluation***"
      ],
      "metadata": {
        "id": "y6H_PFlRINKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classifier(model, loader, ce_criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels in loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            logits = model.forward_inference_10(data)\n",
        "            ce_loss = ce_criterion(logits, labels)\n",
        "\n",
        "            total_loss += ce_loss.item() * data.size(0)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return total_loss/total, correct/total"
      ],
      "metadata": {
        "id": "byaasxxdZipM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Clustering***"
      ],
      "metadata": {
        "id": "V4ZnRkZXIYqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_dbscan_clusters(model, dataset, eps, min_samples):\n",
        "    \"\"\"\n",
        "    For each class c in 0..9, gather embeddings and run DBSCAN with (eps,min_samples).\n",
        "    Return a dict c -> list of (centroid, radius) for each cluster.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    label2vecs = {c: [] for c in range(10)}\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
        "    with torch.no_grad():\n",
        "        for data, labels in loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            emb = model.forward_features(data)\n",
        "            emb_np = emb.cpu().numpy()  # shape (B,embedding_dim)\n",
        "            for i, lab in enumerate(labels):\n",
        "                label2vecs[lab.item()].append(emb_np[i])\n",
        "\n",
        "    dbscan_dict = {}\n",
        "    for c in range(10):\n",
        "        arr = np.vstack(label2vecs[c])  # (samples_in_class,embedding_dim)\n",
        "        if len(arr) == 0:\n",
        "            dbscan_dict[c] = []\n",
        "            continue\n",
        "\n",
        "        dbs = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        cluster_labels = dbs.fit_predict(arr)\n",
        "\n",
        "        clusters_info = []\n",
        "        unique_labels = set(cluster_labels) - {-1}\n",
        "        for clab in unique_labels:\n",
        "            points_in_cluster = arr[cluster_labels == clab]\n",
        "            centroid = points_in_cluster.mean(axis=0)\n",
        "            dists = np.sqrt(((points_in_cluster - centroid)**2).sum(axis=1))\n",
        "            radius = dists.max()\n",
        "            clusters_info.append({\n",
        "                \"centroid\": centroid,\n",
        "                \"radius\": radius\n",
        "            })\n",
        "        dbscan_dict[c] = clusters_info\n",
        "\n",
        "    return dbscan_dict\n"
      ],
      "metadata": {
        "id": "lwRqS5NzvouK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Wrapping***"
      ],
      "metadata": {
        "id": "Gm7PbCCTIwYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OSRWrapperDBSCAN(nn.Module):\n",
        "    def __init__(self, core_model, dbscan_dict, prob_threshold=0.7, dist_factor=1.0):\n",
        "        \"\"\"\n",
        "        1) If maxProb < prob_threshold => unknown\n",
        "        2) Else find nearest cluster (by centroid) in predicted class -> check distance < radius*dist_factor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.core_model = core_model\n",
        "        self.dbscan_dict = dbscan_dict\n",
        "        self.prob_threshold = prob_threshold\n",
        "        self.dist_factor = dist_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.core_model.forward_features(x)\n",
        "        logits_10 = self.core_model.forward_classifier(emb)\n",
        "        probs_10 = F.softmax(logits_10, dim=1)\n",
        "\n",
        "        out_11 = torch.zeros(x.size(0), 11, device=x.device)\n",
        "        out_11[:, :10] = logits_10\n",
        "\n",
        "        max_probs, pred_c = torch.max(probs_10, dim=1)\n",
        "        emb_np = emb.detach().cpu().numpy()\n",
        "\n",
        "        for i in range(x.size(0)):\n",
        "            p = max_probs[i].item()\n",
        "            c = pred_c[i].item()\n",
        "\n",
        "            if p < self.prob_threshold:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "                continue\n",
        "\n",
        "            clusters_info = self.dbscan_dict[c]\n",
        "            if len(clusters_info) == 0:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "                continue\n",
        "\n",
        "            sample_emb = emb_np[i]\n",
        "            min_dist = 1e9\n",
        "            best_rad = 0.0\n",
        "            for info in clusters_info:\n",
        "                centroid = info[\"centroid\"]\n",
        "                radius = info[\"radius\"]\n",
        "                dist = np.sqrt(((sample_emb - centroid)**2).sum())\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    best_rad = radius\n",
        "\n",
        "            if min_dist > best_rad*self.dist_factor:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "            else:\n",
        "                out_11[i, 10] = -10.0\n",
        "\n",
        "        return out_11"
      ],
      "metadata": {
        "id": "s_cTRxgivrVX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***MNIST: In-Distribution Dataset***"
      ],
      "metadata": {
        "id": "e0FouooOe5hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms for MNIST\n",
        "transform_mnist_train = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "transform_mnist_test = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist_train_full = MNIST(root='./data', train=True, download=True, transform=transform_mnist_train)\n",
        "mnist_test = MNIST(root='./data', train=False, download=True, transform=transform_mnist_test)\n",
        "\n",
        "# Split MNIST training set into train and validation\n",
        "val_ratio = 0.1\n",
        "train_size = int((1 - val_ratio) * len(mnist_train_full))\n",
        "val_size = len(mnist_train_full) - train_size\n",
        "\n",
        "mnist_train_ds, mnist_val_ds = torch.utils.data.random_split(\n",
        "    mnist_train_full, [train_size, val_size], generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "# Override transform for validation set\n",
        "mnist_val_ds.dataset.transform = transform_mnist_test\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(mnist_train_ds, batch_size=batch_size, shuffle=True)\n",
        "mnist_val_loader = DataLoader(mnist_val_ds, batch_size=batch_size, shuffle=False)\n",
        "mnist_test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"MNIST datasets loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW_vKhDFe4vO",
        "outputId": "af25b59c-8c4c-4a65-ac9d-16d709b66977",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 165MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 39.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 40.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.57MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "MNIST datasets loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Load ood_dataset and combine***"
      ],
      "metadata": {
        "id": "xDfouPpQTqe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ood_dataset(ood_test):\n",
        "  # Randomly select OOD samples from the chosen dataset\n",
        "  inds = list(range(len(ood_test)))\n",
        "  random.shuffle(inds)\n",
        "\n",
        "  inds_ood_val = inds[:num_ood_val]\n",
        "  inds_ood_test = inds[num_ood_val: num_ood_val + num_ood_test]\n",
        "\n",
        "  ood_val_ds = Subset(ood_test, inds_ood_val)\n",
        "  ood_test_ds = Subset(ood_test, inds_ood_test)\n",
        "\n",
        "  val_ood_ds = CombineDataset(mnist_val_ds, ood_val_ds)\n",
        "  test_ood_ds = CombineDataset(mnist_test, ood_test_ds)\n",
        "\n",
        "  val_ood_loader = DataLoader(val_ood_ds, batch_size=256, shuffle=False)\n",
        "  test_ood_loader = DataLoader(test_ood_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "  return val_ood_loader, test_ood_loader"
      ],
      "metadata": {
        "id": "YL7PWiuihtxO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Load ood_dataSet***"
      ],
      "metadata": {
        "id": "m8SUI4VmTGqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common transformation for CIFAR10, EMNIST, FashionMNIST, and OtherDataset\n",
        "transform_ood = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.Grayscale(num_output_channels=1),  # Convert RGB to grayscale\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Select which ood to test on\n",
        "# ======================================================================\n",
        "\n",
        "# Load CIFAR10 as OOD dataset\n",
        "# cifar_test = CIFAR10(root='./data', train=False, download=True, transform=transform_ood)\n",
        "# ood_test = cifar_test\n",
        "# ood_title = \"cifar10\"\n",
        "\n",
        "# Load EMNIST as OOD dataset\n",
        "# emnist_test = EMNIST(root='./data', split=\"balanced\", train=False, download=True, transform=transform_ood)\n",
        "# ood_test = emnist_test\n",
        "# ood_title = \"emnist\"\n",
        "\n",
        "# Load fashionMNIST as OOD dataset\n",
        "fashion_mnist_test = FashionMNIST(root='./data', train=False, download=True, transform=transform_ood)\n",
        "ood_test = fashion_mnist_test\n",
        "ood_title = \"fashionMNIST\"\n",
        "\n",
        "# ======================================================================\n",
        "# Load osr_dataset as OOD dataset\n",
        "# osr_dataset = osr_dataset(root='./data', train=False, download=True, transform=transform_ood)  # Replace this with your actual dataset\n",
        "# ood_test = osr_dataset\n",
        "# ood_title = \"osr_dataset\"\n",
        "# ======================================================================\n",
        "\n",
        "ood_val_loader, ood_test_loader = load_ood_dataset(ood_test)\n",
        "print(f\"Loaded {ood_title} as OOD dataset.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvwGVVWXfIg9",
        "outputId": "1e88569b-e984-4a98-c73c-32116333b5b8",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 21.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 343kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.23MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 12.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Loaded fashionMNIST as OOD dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Plot Training and Validation Losses***"
      ],
      "metadata": {
        "id": "T41zPZ1EnAQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_trainingAndValidationLoss(train_losses, val_losses):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
        "    plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "J9lNBDipm_Ph"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Train and Evaluate stage***"
      ],
      "metadata": {
        "id": "L6wgt1UqUAye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "core_model = CNN_EmbeddingNet_Triplet(embedding_dim=embedding_dim, dropout_p=0.3).to(device)\n",
        "optimizer = optim.Adam(core_model.parameters(), lr=lr)\n",
        "ce_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Lists to store training and validation losses for plotting\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "if not eval_mode:\n",
        "    print(\"=== Training CNN Embedding Model (Triplet+CE) on MNIST ===\")\n",
        "    for ep in range(1, epochs+1):\n",
        "        total_loss, ce_val, trip_val, train_acc = train_one_epoch_triplet(\n",
        "            core_model, train_loader, optimizer, ce_criterion,\n",
        "            triplet_margin=1.0, alpha=triplet_alpha\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_classifier(core_model, mnist_val_loader, ce_criterion)\n",
        "\n",
        "        train_losses.append(total_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {ep}/{epochs} | \"\n",
        "              f\"TrainLoss={total_loss:.4f} (CE={ce_val:.4f}, T={trip_val:.4f}), \"\n",
        "              f\"Acc={train_acc:.4f} | ValLoss={val_loss:.4f}, ValAcc={val_acc:.4f}\")\n",
        "\n",
        "    torch.save(core_model.state_dict(), \"mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "    print(\"Saved weights to mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "\n",
        "    plot_trainingAndValidationLoss(train_losses, val_losses)\n",
        "else:\n",
        "    core_model.load_state_dict(torch.load(\"mnist_cnn_triplet_dbscan_tuning.pth\", map_location=device))\n",
        "    print(\"Loaded weights from mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "\n"
      ],
      "metadata": {
        "id": "m2apW4VXZyEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21348a3-b86c-4181-f8a7-bcdc1310fb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training CNN Embedding Model (Triplet+CE) on MNIST ===\n",
            "Epoch 1/5 | TrainLoss=0.4294 (CE=0.2787, T=0.1507), Acc=0.9162 | ValLoss=0.0654, ValAcc=0.9817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hyperparameter Tuning***"
      ],
      "metadata": {
        "id": "Z5RQ9-IeUIVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if eval_mode:\n",
        "    core_model.load_state_dict(torch.load(\"mnist_cnn_triplet_dbscan_tuning.pth\", map_location=device))\n",
        "    print(\"Loaded weights from mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "\n",
        "else:\n",
        "  eps_grid = [1.5, 2.0, 2.5]\n",
        "  min_samples_grid = [3, 5]\n",
        "  prob_grid = [0.6, 0.7, 0.8]\n",
        "  dist_factor_grid = [1.0, 1.2]\n",
        "\n",
        "  best_config = None\n",
        "  best_acc = 0.0\n",
        "\n",
        "  print(\"\\n=== Hyperparameter Tuning over DBSCAN + prob_threshold + dist_factor ===\")\n",
        "  for eps_val in eps_grid:\n",
        "      for min_s in min_samples_grid:\n",
        "          # compute DBSCAN clusters for current combination\n",
        "          dbscan_dict = compute_dbscan_clusters(core_model, mnist_train_ds, eps=eps_val, min_samples=min_s)\n",
        "          for prob_t in prob_grid:\n",
        "              for dist_f in dist_factor_grid:\n",
        "                  # build a wrapper model\n",
        "                  temp_model = OSRWrapperDBSCAN(\n",
        "                      core_model,\n",
        "                      dbscan_dict,\n",
        "                      prob_threshold=prob_t,\n",
        "                      dist_factor=dist_f\n",
        "                  ).to(device)\n",
        "\n",
        "                  acc_mnist, acc_ood, acc_total = eval_model(temp_model, ood_val_loader, device)\n",
        "\n",
        "                  if acc_total > best_acc:\n",
        "                      best_acc = acc_total\n",
        "                      best_config = (eps_val, min_s, prob_t, dist_f)\n",
        "                      print(f\"New best: eps={eps_val}, minS={min_s}, prob={prob_t}, distF={dist_f}, acc_total={acc_total*100:.2f}%\")\n",
        "\n",
        "  eps_best, min_s_best, prob_best, dist_f_best = best_config\n",
        "  print(f\"\\nBest config = eps={eps_best}, min_samples={min_s_best}, prob={prob_best}, dist_factor={dist_f_best}, acc_total={best_acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "RdIPtbCRWLBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Final Evaluation***"
      ],
      "metadata": {
        "id": "Y0amxo_PUPFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  compute DBSCAN again with best config, and evaluate on test set\n",
        "dbscan_dict_final = compute_dbscan_clusters(core_model, mnist_train_ds, eps=eps_best, min_samples=min_s_best)\n",
        "final_model = OSRWrapperDBSCAN(core_model, dbscan_dict_final, prob_threshold=prob_best, dist_factor=dist_f_best).to(device)\n",
        "\n",
        "acc_mnist_test, acc_ood_test, acc_total_test = eval_model(final_model, ood_test_loader, device)\n",
        "print(\"\\n=== Final OSR (with best config) on Test ===\")\n",
        "print(f\"Accuracy on MNIST: {acc_mnist_test*100:.2f}%\")\n",
        "print(f\"Accuracy on OOD ({ood_title}):   {acc_ood_test*100:.2f}%\")\n",
        "print(f\"Total Accuracy:    {acc_total_test*100:.2f}%\")\n",
        "\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "id": "1XYRFE-TwDpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Confusion Matrix for 11 classes (0-9 + Unknown)***"
      ],
      "metadata": {
        "id": "8WnxA-5kliWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for data, labels in ood_test_loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        outputs = final_model(data)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "all_preds = np.concatenate(all_preds)\n",
        "all_labels = np.concatenate(all_labels)\n",
        "\n",
        "cm_11 = confusion_matrix(all_labels, all_preds, labels=list(range(10)) + [10])\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_11, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=list(range(10)) + ['Unknown'],\n",
        "            yticklabels=list(range(10)) + ['Unknown'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (11 Classes)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jhR9-_znnRCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Binary Confusion Matrix (Known vs Unknown)***"
      ],
      "metadata": {
        "id": "MXuKnIvqnqW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_preds = np.where(all_preds < 10, 0, 1)\n",
        "binary_labels = np.where(all_labels < 10, 0, 1)\n",
        "cm_bin = confusion_matrix(binary_labels, binary_preds, labels=[0, 1])\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_bin, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=['Known', 'Unknown'],\n",
        "            yticklabels=['Known', 'Unknown'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Binary Confusion Matrix (Known vs Unknown)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "le1AqOIqnqpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***t-SNE Visualization of Embeddings from Test Set***"
      ],
      "metadata": {
        "id": "FmBxgSe3nq2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_embeddings = []\n",
        "all_labels_emb = []\n",
        "with torch.no_grad():\n",
        "    for data, labels in ood_test_loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        emb = final_model.core_model.forward_features(data)\n",
        "        all_embeddings.append(emb.cpu().numpy())\n",
        "        all_labels_emb.append(labels.cpu().numpy())\n",
        "all_embeddings = np.concatenate(all_embeddings)\n",
        "all_labels_emb = np.concatenate(all_labels_emb)\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "embeddings_2d = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
        "colors = np.vstack([colors, np.array([0.5, 0.5, 0.5, 1])])  # Color for Unknown\n",
        "for label in np.unique(all_labels_emb):\n",
        "    idx = all_labels_emb == label\n",
        "    plt.scatter(embeddings_2d[idx, 0], embeddings_2d[idx, 1],\n",
        "                color=colors[label] if label < 10 else colors[-1],\n",
        "                label=str(label) if label < 10 else 'Unknown',\n",
        "                alpha=0.6)\n",
        "plt.legend()\n",
        "plt.title('t-SNE Visualization of Embeddings')\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "niAXErLZnrOo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}