{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Omerba31/Deep/blob/main/final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***imports***"
      ],
      "metadata": {
        "id": "OxD7EpUX__Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
        "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10, EMNIST\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.cluster import DBSCAN"
      ],
      "metadata": {
        "id": "FHaWSKkKY9kJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Configuration and Environment Setup***"
      ],
      "metadata": {
        "id": "1XW1ao2MAinu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "eval_mode = False\n",
        "epochs = 5\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "embedding_dim = 64\n",
        "triplet_alpha = 1.0  # Weight for Triplet Loss\n",
        "\n",
        "num_ood_val = 300\n",
        "num_ood_test = 800"
      ],
      "metadata": {
        "id": "QVqL7KPMZIlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01f417d7-60b6-478a-bc09-32ff83b26530"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Datasets Combination***"
      ],
      "metadata": {
        "id": "jXKHMwqMArcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedDataset(Dataset):\n",
        "\n",
        "    \"\"\"for OOD: label=10 \"\"\"\n",
        "\n",
        "    def __init__(self, ds_in, ds_ood):\n",
        "        self.ds_in = ds_in\n",
        "        self.ds_ood = ds_ood\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds_in) + len(self.ds_ood)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < len(self.ds_in):\n",
        "            data, label = self.ds_in[idx]\n",
        "            return data, label\n",
        "        else:\n",
        "            data, _ = self.ds_ood[idx - len(self.ds_in)]\n",
        "            return data, 10"
      ],
      "metadata": {
        "id": "Ab9Y6HYVZVZP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Evaluation Function***"
      ],
      "metadata": {
        "id": "rHG7-2fMCG3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, data_loader, device):\n",
        "\n",
        "    model.eval()\n",
        "    correct_mnist = 0\n",
        "    total_mnist = 0\n",
        "    correct_ood = 0\n",
        "    total_ood = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in data_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)  # shape (N,11)\n",
        "            _, y_pred = torch.max(outputs, dim=1)\n",
        "\n",
        "            mask_in = (labels < 10) # MNIST\n",
        "            mask_ood = (labels == 10) # OOD\n",
        "\n",
        "            labels_in = labels[mask_in]\n",
        "            labels_ood = labels[mask_ood]\n",
        "            pred_in = y_pred[mask_in]\n",
        "            pred_ood = y_pred[mask_ood]\n",
        "\n",
        "            total_mnist += labels_in.size(0)\n",
        "            correct_mnist += (pred_in == labels_in).sum().item()\n",
        "\n",
        "            total_ood += labels_ood.size(0)\n",
        "            correct_ood += (pred_ood == labels_ood).sum().item()\n",
        "\n",
        "    acc_mnist = correct_mnist / total_mnist if total_mnist>0 else 0 # mnist Accuracy\n",
        "    acc_ood   = correct_ood / total_ood     if total_ood>0   else 0 # ood Accuracy\n",
        "    acc_total = (correct_mnist + correct_ood) / (total_mnist + total_ood) # overall Accuracy\n",
        "    return acc_mnist, acc_ood, acc_total\n"
      ],
      "metadata": {
        "id": "-g4x7uq6ZXtw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Net initialization***"
      ],
      "metadata": {
        "id": "Gweo_aLAHqFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_EmbeddingNet_Triplet(nn.Module):\n",
        "    def __init__(self, embedding_dim=64, dropout_p=0.3):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        # Convs\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "        # FC\n",
        "        self.fc1 = nn.Linear(64*7*7, 128)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc2 = nn.Linear(128, embedding_dim)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(embedding_dim, 10)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        emb = self.fc2(x)\n",
        "        return emb\n",
        "\n",
        "    def forward_classifier(self, emb):\n",
        "        return self.classifier(emb)\n",
        "\n",
        "    def forward_inference_10(self, x):\n",
        "        emb = self.forward_features(x)\n",
        "        logits = self.forward_classifier(emb)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.forward_inference_10(x)\n"
      ],
      "metadata": {
        "id": "7WtnpjA-dVvC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Train***"
      ],
      "metadata": {
        "id": "u6XTrBeeH_6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch_triplet(model, loader, optimizer, ce_criterion, triplet_margin=1.0, alpha=1.0):\n",
        "    model.train()\n",
        "    triplet_criterion = nn.TripletMarginLoss(margin=triplet_margin, reduction='mean')\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_ce   = 0.0\n",
        "    running_trip = 0.0\n",
        "    total_samples = 0\n",
        "    correct = 0\n",
        "\n",
        "    for data, labels in loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model.forward_inference_10(data)\n",
        "        ce_loss = ce_criterion(logits, labels)\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        emb = model.forward_features(data)\n",
        "\n",
        "        anchor_list = []\n",
        "        pos_list = []\n",
        "        neg_list = []\n",
        "\n",
        "        label2indices = {}\n",
        "        for i, lab in enumerate(labels):\n",
        "            lab_i = lab.item()\n",
        "            label2indices.setdefault(lab_i, []).append(i)\n",
        "\n",
        "        for i, lab in enumerate(labels):\n",
        "            anchor_label = lab.item()\n",
        "            if len(label2indices[anchor_label]) < 2:\n",
        "                continue\n",
        "            pos_idx = i\n",
        "            while pos_idx == i:\n",
        "                pos_idx = random.choice(label2indices[anchor_label])\n",
        "\n",
        "            neg_label = anchor_label\n",
        "            while neg_label == anchor_label:\n",
        "                neg_label = random.choice(list(label2indices.keys()))\n",
        "            neg_idx = random.choice(label2indices[neg_label])\n",
        "\n",
        "            anchor_list.append(emb[i].unsqueeze(0))\n",
        "            pos_list.append(emb[pos_idx].unsqueeze(0))\n",
        "            neg_list.append(emb[neg_idx].unsqueeze(0))\n",
        "\n",
        "        if len(anchor_list) > 0:\n",
        "            anchor_t = torch.cat(anchor_list, dim=0)\n",
        "            pos_t = torch.cat(pos_list, dim=0)\n",
        "            neg_t = torch.cat(neg_list, dim=0)\n",
        "            trip_loss = triplet_criterion(anchor_t, pos_t, neg_t)\n",
        "        else:\n",
        "            trip_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "        total_loss = ce_loss + alpha*trip_loss\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += total_loss.item() * data.size(0)\n",
        "        running_ce   += ce_loss.item() * data.size(0)\n",
        "        running_trip += trip_loss.item() * data.size(0)\n",
        "\n",
        "    ep_loss = running_loss / total_samples\n",
        "    ep_ce   = running_ce / total_samples\n",
        "    ep_trip = running_trip / total_samples\n",
        "    ep_acc  = correct / total_samples\n",
        "    return ep_loss, ep_ce, ep_trip, ep_acc\n"
      ],
      "metadata": {
        "id": "nHkGcoaIZhAH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Classifier Evaluation***"
      ],
      "metadata": {
        "id": "y6H_PFlRINKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classifier(model, loader, ce_criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels in loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            logits = model.forward_inference_10(data)\n",
        "            ce_loss = ce_criterion(logits, labels)\n",
        "\n",
        "            total_loss += ce_loss.item() * data.size(0)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return total_loss/total, correct/total"
      ],
      "metadata": {
        "id": "byaasxxdZipM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Clustering***"
      ],
      "metadata": {
        "id": "V4ZnRkZXIYqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_dbscan_clusters(model, dataset, eps, min_samples):\n",
        "    \"\"\"\n",
        "    For each class c in 0..9, gather embeddings and run DBSCAN with (eps,min_samples).\n",
        "    Return a dict c -> list of (centroid, radius) for each cluster.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    label2vecs = {c: [] for c in range(10)}\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
        "    with torch.no_grad():\n",
        "        for data, labels in loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            emb = model.forward_features(data)\n",
        "            emb_np = emb.cpu().numpy()  # shape (B,embedding_dim)\n",
        "            for i, lab in enumerate(labels):\n",
        "                label2vecs[lab.item()].append(emb_np[i])\n",
        "\n",
        "    dbscan_dict = {}\n",
        "    for c in range(10):\n",
        "        arr = np.vstack(label2vecs[c])  # (samples_in_class,embedding_dim)\n",
        "        if len(arr) == 0:\n",
        "            dbscan_dict[c] = []\n",
        "            continue\n",
        "\n",
        "        dbs = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        cluster_labels = dbs.fit_predict(arr)\n",
        "\n",
        "        clusters_info = []\n",
        "        unique_labels = set(cluster_labels) - {-1}\n",
        "        for clab in unique_labels:\n",
        "            points_in_cluster = arr[cluster_labels == clab]\n",
        "            centroid = points_in_cluster.mean(axis=0)\n",
        "            dists = np.sqrt(((points_in_cluster - centroid)**2).sum(axis=1))\n",
        "            radius = dists.max()\n",
        "            clusters_info.append({\n",
        "                \"centroid\": centroid,\n",
        "                \"radius\": radius\n",
        "            })\n",
        "        dbscan_dict[c] = clusters_info\n",
        "\n",
        "    return dbscan_dict\n"
      ],
      "metadata": {
        "id": "lwRqS5NzvouK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Wrapping***"
      ],
      "metadata": {
        "id": "Gm7PbCCTIwYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OSRWrapperDBSCAN(nn.Module):\n",
        "    def __init__(self, core_model, dbscan_dict, prob_threshold=0.7, dist_factor=1.0):\n",
        "        \"\"\"\n",
        "        1) If maxProb < prob_threshold => unknown\n",
        "        2) Else find nearest cluster (by centroid) in predicted class -> check distance < radius*dist_factor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.core_model = core_model\n",
        "        self.dbscan_dict = dbscan_dict\n",
        "        self.prob_threshold = prob_threshold\n",
        "        self.dist_factor = dist_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.core_model.forward_features(x)\n",
        "        logits_10 = self.core_model.forward_classifier(emb)\n",
        "        probs_10 = F.softmax(logits_10, dim=1)\n",
        "\n",
        "        out_11 = torch.zeros(x.size(0), 11, device=x.device)\n",
        "        out_11[:, :10] = logits_10\n",
        "\n",
        "        max_probs, pred_c = torch.max(probs_10, dim=1)\n",
        "        emb_np = emb.detach().cpu().numpy()\n",
        "\n",
        "        for i in range(x.size(0)):\n",
        "            p = max_probs[i].item()\n",
        "            c = pred_c[i].item()\n",
        "\n",
        "            if p < self.prob_threshold:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "                continue\n",
        "\n",
        "            clusters_info = self.dbscan_dict[c]\n",
        "            if len(clusters_info) == 0:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "                continue\n",
        "\n",
        "            sample_emb = emb_np[i]\n",
        "            min_dist = 1e9\n",
        "            best_rad = 0.0\n",
        "            for info in clusters_info:\n",
        "                centroid = info[\"centroid\"]\n",
        "                radius = info[\"radius\"]\n",
        "                dist = np.sqrt(((sample_emb - centroid)**2).sum())\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    best_rad = radius\n",
        "\n",
        "            if min_dist > best_rad*self.dist_factor:\n",
        "                out_11[i, c] = -10.0\n",
        "                out_11[i, 10] = 10.0\n",
        "            else:\n",
        "                out_11[i, 10] = -10.0\n",
        "\n",
        "        return out_11"
      ],
      "metadata": {
        "id": "s_cTRxgivrVX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Data Preparation and Augmentation***"
      ],
      "metadata": {
        "id": "Pp85JQRSJH-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((28,28)),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(0, translate=(0.1,0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((28,28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "mnist_train_full = MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
        "mnist_test       = MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "val_ratio = 0.1\n",
        "train_size = int((1 - val_ratio)*len(mnist_train_full))\n",
        "val_size   = len(mnist_train_full) - train_size\n",
        "mnist_train_ds, mnist_val_ds = random_split(mnist_train_full, [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(SEED)\n",
        ")\n",
        "mnist_val_ds.dataset.transform = transform_test\n",
        "\n",
        "train_loader = DataLoader(mnist_train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(mnist_val_ds,   batch_size=batch_size, shuffle=False)\n",
        "\n",
        "fashion_ds = FashionMNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "inds = list(range(len(fashion_ds)))\n",
        "random.shuffle(inds)\n",
        "inds_val  = inds[:num_ood_val]\n",
        "inds_test = inds[num_ood_val : num_ood_val + num_ood_test]\n",
        "\n",
        "fashion_ood_val_ds  = Subset(fashion_ds, inds_val)\n",
        "fashion_ood_test_ds = Subset(fashion_ds, inds_test)\n",
        "\n",
        "val_ood_ds  = CombinedDataset(mnist_val_ds,  fashion_ood_val_ds)\n",
        "test_ood_ds = CombinedDataset(mnist_test,    fashion_ood_test_ds)\n",
        "\n",
        "val_ood_loader  = DataLoader(val_ood_ds,  batch_size=256, shuffle=False)\n",
        "test_ood_loader = DataLoader(test_ood_ds, batch_size=256, shuffle=False)\n"
      ],
      "metadata": {
        "id": "cJ7Ck5dWZu0q",
        "collapsed": true
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Transform and load MNIST as in-dist***"
      ],
      "metadata": {
        "id": "6hdP9WU3Lmy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "#   Transforms\n",
        "# ======================================================================\n",
        "transform_mnist_train = transforms.Compose([\n",
        "    transforms.Resize((28,28)),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(0, translate=(0.1,0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "transform_mnist_test = transforms.Compose([\n",
        "    transforms.Resize((28,28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# ======================================================================\n",
        "#   Loading MNIST as in-dist\n",
        "# ======================================================================\n",
        "mnist_train_full = MNIST(root='./data', train=True, download=True, transform=transform_mnist_train)\n",
        "mnist_test       = MNIST(root='./data', train=False, download=True, transform=transform_mnist_test)\n",
        "\n",
        "# We define a validation split from the training data\n",
        "val_ratio = 0.1\n",
        "train_size = int((1 - val_ratio)*len(mnist_train_full))\n",
        "val_size   = len(mnist_train_full) - train_size\n",
        "mnist_train_ds, mnist_val_ds = torch.utils.data.random_split(\n",
        "    mnist_train_full,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(SEED)\n",
        ")\n",
        "# override transform for val set:\n",
        "mnist_val_ds.dataset.transform = transform_mnist_test\n",
        "\n",
        "train_loader = DataLoader(mnist_train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(mnist_val_ds,   batch_size=batch_size, shuffle=False)\n",
        "mnist_test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "2oNZwk9iL84x"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Transform and load CIFAR10 / FashionMNIST / EMNIST / YOUR_DATASET as out-dist***"
      ],
      "metadata": {
        "id": "3EbmGXWONDEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28,28)),\n",
        "    transforms.Grayscale(num_output_channels=1),  # from RGB to 1 channel\n",
        "    transforms.ToTensor(),\n",
        "    # We can reuse the same mean/std as MNIST or recalc. We'll do the same for simplicity:\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "def load_DATASET(DATASET, root='./data', train=False, download=True, transform=transform):\n",
        "    if (DATASET == \"CIFAR10\"):\n",
        "        return CIFAR10(root, train, download, transform), \"CIFAR10\"\n",
        "    elif (DATASET == \"FashionMNIST\"):\n",
        "        return FashionMNIST(root, train, download, transform), \"FashionMNIST\"\n",
        "    elif (DATASET == \"EMNIST\"):\n",
        "        return EMNIST(root, train, download, transform, split=\"balanced\"), \"EMNIST\"\n",
        "        # OR: split = \"byclass\", \"bymerge\", etc.)\n",
        "\n",
        "  # ======================================================================\n",
        "  #   Loading YOUR_DATASET as out-dist\n",
        "  # ======================================================================\n",
        "    # else:\n",
        "    #     return YOUR_DATASET(root='./data', train=False, download=True, transform=transform), \"YOUR_DATASET_TITLE\"\n"
      ],
      "metadata": {
        "id": "Ton-KNmpNZ_g"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Data preparations***"
      ],
      "metadata": {
        "id": "FbJ3cjwdQ9wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_ood_val = 300\n",
        "num_ood_test = 800\n",
        "\n",
        "ood_type = \"CIFAR10\" # <============= Choose ood_type\n",
        "# ood_type = \"CIFAR10\" / \"FashionMNIST\" / \"EMNIST\" / \"YOUR_DATASET\"\n",
        "\n",
        "ood_test, ood_title = load_DATASET(ood_type)\n",
        "\n",
        "inds = list(range(len(ood_test)))\n",
        "random.shuffle(inds)\n",
        "\n",
        "# separate: OOD for val, OOD for test\n",
        "inds_ood_val  = inds[:num_ood_val]\n",
        "inds_ood_test = inds[num_ood_val : num_ood_val + num_ood_test]\n",
        "\n",
        "ood_val_ds  = Subset(ood_test, inds_ood_val)\n",
        "ood_test_ds = Subset(ood_test, inds_ood_test)\n",
        "\n",
        "# Combined datasets for OSR\n",
        "val_ood_ds  = CombinedDataset(mnist_val_ds,  ood_val_ds)\n",
        "test_ood_ds = CombinedDataset(mnist_test,    ood_test_ds)\n",
        "\n",
        "val_ood_loader  = DataLoader(val_ood_ds,  batch_size=256, shuffle=False)\n",
        "test_ood_loader = DataLoader(test_ood_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "print(\"Datasets loaded. MNIST + \" + ood_title + \" as OOD.\")\n",
        "ood_ds = ood_test\n",
        "ood_val_ds  = ood_val_ds\n",
        "ood_test_ds = ood_test_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN7PD-HE0RXp",
        "outputId": "f2317be7-10e3-45ab-dd43-9aa1eb58ca78"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets loaded. MNIST + CIFAR10 as OOD.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Train and Evaluate stage***"
      ],
      "metadata": {
        "id": "L6wgt1UqUAye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "core_model = CNN_EmbeddingNet_Triplet(embedding_dim=embedding_dim, dropout_p=0.3).to(device)\n",
        "optimizer = optim.Adam(core_model.parameters(), lr=lr)\n",
        "ce_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if not eval_mode:\n",
        "    print(\"=== Training CNN Embedding Model (Triplet+CE) on MNIST ===\")\n",
        "    for ep in range(1, epochs+1):\n",
        "        total_loss, ce_val, trip_val, train_acc = train_one_epoch_triplet(\n",
        "            core_model, train_loader, optimizer, ce_criterion,\n",
        "            triplet_margin=1.0, alpha=triplet_alpha\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_classifier(core_model, val_loader, ce_criterion)\n",
        "        print(f\"Epoch {ep}/{epochs} | \"\n",
        "              f\"TrainLoss={total_loss:.4f} (CE={ce_val:.4f}, T={trip_val:.4f}), \"\n",
        "              f\"Acc={train_acc:.4f} | ValLoss={val_loss:.4f}, ValAcc={val_acc:.4f}\")\n",
        "\n",
        "    torch.save(core_model.state_dict(), \"mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "    print(\"Saved weights to mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "else:\n",
        "    core_model.load_state_dict(torch.load(\"mnist_cnn_triplet_dbscan_tuning.pth\", map_location=device))\n",
        "    print(\"Loaded weights from mnist_cnn_triplet_dbscan_tuning.pth\")\n",
        "\n"
      ],
      "metadata": {
        "id": "m2apW4VXZyEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a5b958-996a-4fa5-8101-499d2c6f8fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training CNN Embedding Model (Triplet+CE) on MNIST ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hyperparameter Tuning***"
      ],
      "metadata": {
        "id": "Z5RQ9-IeUIVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps_grid = [1.5, 2.0, 2.5]\n",
        "min_samples_grid = [3, 5]\n",
        "prob_grid = [0.6, 0.7, 0.8]\n",
        "dist_factor_grid = [1.0, 1.2]\n",
        "\n",
        "best_config = None\n",
        "best_acc = 0.0\n",
        "\n",
        "print(\"\\n=== Hyperparameter Tuning over DBSCAN + prob_threshold + dist_factor ===\")\n",
        "for eps_val in eps_grid:\n",
        "    for min_s in min_samples_grid:\n",
        "        # compute DBSCAN clusters for this combination\n",
        "        dbscan_dict = compute_dbscan_clusters(core_model, mnist_train_ds, eps=eps_val, min_samples=min_s)\n",
        "\n",
        "        for prob_t in prob_grid:\n",
        "            for dist_f in dist_factor_grid:\n",
        "                # build a wrapper model\n",
        "                temp_model = OSRWrapperDBSCAN(\n",
        "                    core_model,\n",
        "                    dbscan_dict,\n",
        "                    prob_threshold=prob_t,\n",
        "                    dist_factor=dist_f\n",
        "                ).to(device)\n",
        "\n",
        "                acc_mnist, acc_ood, acc_total = eval_model(temp_model, val_ood_loader, device)\n",
        "\n",
        "                if acc_total > best_acc:\n",
        "                    best_acc = acc_total\n",
        "                    best_config = (eps_val, min_s, prob_t, dist_f)\n",
        "                    print(f\"New best: eps={eps_val}, minS={min_s}, prob={prob_t}, distF={dist_f}, valAcc={acc_total*100:.2f}%\")\n",
        "\n",
        "eps_best, min_s_best, prob_best, dist_f_best = best_config\n",
        "print(f\"\\nBest config = eps={eps_best}, min_samples={min_s_best}, prob={prob_best}, dist_factor={dist_f_best}, valAcc={best_acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "RdIPtbCRWLBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Final Evaluation***"
      ],
      "metadata": {
        "id": "Y0amxo_PUPFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  compute DBSCAN again with best config, and evaluate on test set\n",
        "dbscan_dict_final = compute_dbscan_clusters(core_model, mnist_train_ds, eps=eps_best, min_samples=min_s_best)\n",
        "final_model = OSRWrapperDBSCAN(core_model, dbscan_dict_final, prob_threshold=prob_best, dist_factor=dist_f_best).to(device)\n",
        "\n",
        "acc_mnist_test, acc_ood_test, acc_total_test = eval_model(final_model, test_ood_loader, device)\n",
        "print(\"\\n=== Final OSR (with best config) on Test ===\")\n",
        "print(f\"Accuracy on MNIST: {acc_mnist_test*100:.2f}%\")\n",
        "print(f\"Accuracy on OOD:   {acc_ood_test*100:.2f}%\")\n",
        "print(f\"Total Accuracy:    {acc_total_test*100:.2f}%\")\n",
        "\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "id": "1XYRFE-TwDpk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}